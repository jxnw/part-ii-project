Using SCRIPTS_ROOTDIR: /home/jwang/github/mosesdecoder/scripts
Using single-thread GIZA
using gzip 
(1) preparing corpus @ Fri Oct 15 16:49:25 BST 2021
Executing: mkdir -p /home/jwang/github/part-ii-project/working/exp2/train/corpus
(1.0) selecting factors @ Fri Oct 15 16:49:25 BST 2021
(1.1) running mkcls  @ Fri Oct 15 16:49:25 BST 2021
/home/jwang/github/mosesdecoder/tools/mkcls -c50 -n2 -p/home/jwang/github/part-ii-project/corpus/training/fce.train.gold.bea19.clean.or -V/home/jwang/github/part-ii-project/working/exp2/train/corpus/or.vcb.classes opt
  /home/jwang/github/part-ii-project/working/exp2/train/corpus/or.vcb.classes already in place, reusing
(1.1) running mkcls  @ Fri Oct 15 16:49:25 BST 2021
/home/jwang/github/mosesdecoder/tools/mkcls -c50 -n2 -p/home/jwang/github/part-ii-project/corpus/training/fce.train.gold.bea19.clean.co -V/home/jwang/github/part-ii-project/working/exp2/train/corpus/co.vcb.classes opt
  /home/jwang/github/part-ii-project/working/exp2/train/corpus/co.vcb.classes already in place, reusing
(1.2) creating vcb file /home/jwang/github/part-ii-project/working/exp2/train/corpus/or.vcb @ Fri Oct 15 16:49:26 BST 2021
(1.2) creating vcb file /home/jwang/github/part-ii-project/working/exp2/train/corpus/co.vcb @ Fri Oct 15 16:49:26 BST 2021
(1.3) numberizing corpus /home/jwang/github/part-ii-project/working/exp2/train/corpus/or-co-int-train.snt @ Fri Oct 15 16:49:26 BST 2021
(1.3) numberizing corpus /home/jwang/github/part-ii-project/working/exp2/train/corpus/co-or-int-train.snt @ Fri Oct 15 16:49:26 BST 2021
(2) running giza @ Fri Oct 15 16:49:27 BST 2021
(2.1a) running snt2cooc or-co @ Fri Oct 15 16:49:27 BST 2021

Executing: mkdir -p /home/jwang/github/part-ii-project/working/exp2/train/giza.or-co
Executing: /home/jwang/github/mosesdecoder/tools/snt2cooc.out /home/jwang/github/part-ii-project/working/exp2/train/corpus/co.vcb /home/jwang/github/part-ii-project/working/exp2/train/corpus/or.vcb /home/jwang/github/part-ii-project/working/exp2/train/corpus/or-co-int-train.snt > /home/jwang/github/part-ii-project/working/exp2/train/giza.or-co/or-co.cooc
/home/jwang/github/mosesdecoder/tools/snt2cooc.out /home/jwang/github/part-ii-project/working/exp2/train/corpus/co.vcb /home/jwang/github/part-ii-project/working/exp2/train/corpus/or.vcb /home/jwang/github/part-ii-project/working/exp2/train/corpus/or-co-int-train.snt > /home/jwang/github/part-ii-project/working/exp2/train/giza.or-co/or-co.cooc
line 1000
line 2000
line 3000
line 4000
line 5000
line 6000
line 7000
line 8000
line 9000
line 10000
line 11000
line 12000
line 13000
line 14000
line 15000
line 16000
line 17000
line 18000
line 19000
line 20000
line 21000
line 22000
line 23000
line 24000
line 25000
line 26000
line 27000
line 28000
END.
(2.1b) running giza or-co @ Fri Oct 15 16:49:30 BST 2021
/home/jwang/github/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/jwang/github/part-ii-project/working/exp2/train/giza.or-co/or-co.cooc -c /home/jwang/github/part-ii-project/working/exp2/train/corpus/or-co-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/jwang/github/part-ii-project/working/exp2/train/giza.or-co/or-co -onlyaldumps 1 -p0 0.999 -s /home/jwang/github/part-ii-project/working/exp2/train/corpus/co.vcb -t /home/jwang/github/part-ii-project/working/exp2/train/corpus/or.vcb
Executing: /home/jwang/github/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/jwang/github/part-ii-project/working/exp2/train/giza.or-co/or-co.cooc -c /home/jwang/github/part-ii-project/working/exp2/train/corpus/or-co-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/jwang/github/part-ii-project/working/exp2/train/giza.or-co/or-co -onlyaldumps 1 -p0 0.999 -s /home/jwang/github/part-ii-project/working/exp2/train/corpus/co.vcb -t /home/jwang/github/part-ii-project/working/exp2/train/corpus/or.vcb
/home/jwang/github/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/jwang/github/part-ii-project/working/exp2/train/giza.or-co/or-co.cooc -c /home/jwang/github/part-ii-project/working/exp2/train/corpus/or-co-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/jwang/github/part-ii-project/working/exp2/train/giza.or-co/or-co -onlyaldumps 1 -p0 0.999 -s /home/jwang/github/part-ii-project/working/exp2/train/corpus/co.vcb -t /home/jwang/github/part-ii-project/working/exp2/train/corpus/or.vcb
Parameter 'coocurrencefile' changed from '' to '/home/jwang/github/part-ii-project/working/exp2/train/giza.or-co/or-co.cooc'
Parameter 'c' changed from '' to '/home/jwang/github/part-ii-project/working/exp2/train/corpus/or-co-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '2021-10-15.164930.jwang' to '/home/jwang/github/part-ii-project/working/exp2/train/giza.or-co/or-co'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/jwang/github/part-ii-project/working/exp2/train/corpus/co.vcb'
Parameter 't' changed from '' to '/home/jwang/github/part-ii-project/working/exp2/train/corpus/or.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2021-10-15.164930.jwang.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/jwang/github/part-ii-project/working/exp2/train/giza.or-co/or-co  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/jwang/github/part-ii-project/working/exp2/train/corpus/or-co-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/jwang/github/part-ii-project/working/exp2/train/corpus/co.vcb  (source vocabulary file name)
t = /home/jwang/github/part-ii-project/working/exp2/train/corpus/or.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2021-10-15.164930.jwang.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/jwang/github/part-ii-project/working/exp2/train/giza.or-co/or-co  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/jwang/github/part-ii-project/working/exp2/train/corpus/or-co-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/jwang/github/part-ii-project/working/exp2/train/corpus/co.vcb  (source vocabulary file name)
t = /home/jwang/github/part-ii-project/working/exp2/train/corpus/or.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/jwang/github/part-ii-project/working/exp2/train/corpus/co.vcb
Reading vocabulary file from:/home/jwang/github/part-ii-project/working/exp2/train/corpus/or.vcb
Source vocabulary list has 10816 unique tokens 
Target vocabulary list has 13999 unique tokens 
Calculating vocabulary frequencies from corpus /home/jwang/github/part-ii-project/working/exp2/train/corpus/or-co-int-train.snt
Reading more sentence pairs into memory ... 
Corpus fits in memory, corpus has: 28327 sentence pairs.
 Train total # sentence pairs (weighted): 28327
Size of source portion of the training corpus: 457110 tokens
Size of the target portion of the training corpus: 453015 tokens 
In source portion of the training corpus, only 10815 unique tokens appeared
In target portion of the training corpus, only 13997 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 453015/(485437-28327)== 0.991042
There are 1498622 1498622 entries in table
==========================================================
Model1 Training Started at: Fri Oct 15 16:49:31 2021

-----------
Model1: Iteration 1
Model1: (1) TRAIN CROSS-ENTROPY 13.9799 PERPLEXITY 16157.2
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 18.368 PERPLEXITY 338315
Model 1 Iteration: 1 took: 1 seconds
-----------
Model1: Iteration 2
Model1: (2) TRAIN CROSS-ENTROPY 6.82004 PERPLEXITY 112.989
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 8.73652 PERPLEXITY 426.536
Model 1 Iteration: 2 took: 1 seconds
-----------
Model1: Iteration 3
Model1: (3) TRAIN CROSS-ENTROPY 5.40414 PERPLEXITY 42.3457
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 6.39173 PERPLEXITY 83.9656
Model 1 Iteration: 3 took: 2 seconds
-----------
Model1: Iteration 4
Model1: (4) TRAIN CROSS-ENTROPY 4.92008 PERPLEXITY 30.2756
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 5.56082 PERPLEXITY 47.2035
Model 1 Iteration: 4 took: 3 seconds
-----------
Model1: Iteration 5
Model1: (5) TRAIN CROSS-ENTROPY 4.7723 PERPLEXITY 27.3278
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 5.26611 PERPLEXITY 38.4821
Model 1 Iteration: 5 took: 2 seconds
Entire Model1 Training took: 9 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 0  #classes: 1
Read classes: #words: 0  #classes: 1

==========================================================
Hmm Training Started at: Fri Oct 15 16:49:40 2021

-----------
Hmm: Iteration 1
A/D table contains 178377 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 4.69515 PERPLEXITY 25.9048
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 5.13759 PERPLEXITY 35.2022

Hmm Iteration: 1 took: 20 seconds

-----------
Hmm: Iteration 2
A/D table contains 178377 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 2.07582 PERPLEXITY 4.21584
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 2.12619 PERPLEXITY 4.36563

Hmm Iteration: 2 took: 20 seconds

-----------
Hmm: Iteration 3
A/D table contains 178377 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 1.56739 PERPLEXITY 2.96368
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 1.58868 PERPLEXITY 3.00774

Hmm Iteration: 3 took: 18 seconds

-----------
Hmm: Iteration 4
A/D table contains 178377 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 1.46701 PERPLEXITY 2.76449
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 1.47931 PERPLEXITY 2.78815

Hmm Iteration: 4 took: 6 seconds

-----------
Hmm: Iteration 5
A/D table contains 178377 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 1.42996 PERPLEXITY 2.69438
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 1.43992 PERPLEXITY 2.71306

Hmm Iteration: 5 took: 6 seconds

Entire Hmm Training took: 70 seconds
==========================================================
Read classes: #words: 0  #classes: 1
Read classes: #words: 0  #classes: 1
Read classes: #words: 0  #classes: 1
Read classes: #words: 0  #classes: 1

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Fri Oct 15 16:50:50 2021


---------------------
THTo3: Iteration 1
10000
20000
#centers(pre/hillclimbed/real): 1 1 1  #al: 541.393 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 178377 parameters.
A/D table contains 172250 parameters.
NTable contains 108160 parameter.
p0_count is 436611 and p1 is 8201.78; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 1.21194 PERPLEXITY 2.31649
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 1.21939 PERPLEXITY 2.32849

THTo3 Viterbi Iteration : 1 took: 5 seconds

---------------------
Model3: Iteration 2
10000
20000
#centers(pre/hillclimbed/real): 1 1 1  #al: 541.449 #alsophisticatedcountcollection: 0 #hcsteps: 1.13507
#peggingImprovements: 0
A/D table contains 178377 parameters.
A/D table contains 172250 parameters.
NTable contains 108160 parameter.
p0_count is 440306 and p1 is 6354.53; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 2.00073 PERPLEXITY 4.00203
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 2.00585 PERPLEXITY 4.01626

Model3 Viterbi Iteration : 2 took: 5 seconds

---------------------
Model3: Iteration 3
10000
20000
#centers(pre/hillclimbed/real): 1 1 1  #al: 541.462 #alsophisticatedcountcollection: 0 #hcsteps: 1.13344
#peggingImprovements: 0
A/D table contains 178377 parameters.
A/D table contains 172250 parameters.
NTable contains 108160 parameter.
p0_count is 441172 and p1 is 5921.61; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 1.9785 PERPLEXITY 3.94083
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 1.98287 PERPLEXITY 3.95278

Model3 Viterbi Iteration : 3 took: 4 seconds

---------------------
T3To4: Iteration 4
10000
20000
#centers(pre/hillclimbed/real): 1 1 1  #al: 541.47 #alsophisticatedcountcollection: 6.45984 #hcsteps: 1.13616
#peggingImprovements: 0
D4 table contains 406 parameters.
A/D table contains 178377 parameters.
A/D table contains 172250 parameters.
NTable contains 108160 parameter.
p0_count is 441746 and p1 is 5634.33; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 1.96858 PERPLEXITY 3.91383
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 1.97258 PERPLEXITY 3.9247

T3To4 Viterbi Iteration : 4 took: 5 seconds

---------------------
Model4: Iteration 5
10000
20000
#centers(pre/hillclimbed/real): 1 1 1  #al: 541.464 #alsophisticatedcountcollection: 3.23769 #hcsteps: 1.1011
#peggingImprovements: 0
D4 table contains 406 parameters.
A/D table contains 178377 parameters.
A/D table contains 172112 parameters.
NTable contains 108160 parameter.
p0_count is 441949 and p1 is 5532.92; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 1.639 PERPLEXITY 3.11449
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 1.64124 PERPLEXITY 3.11934

Model4 Viterbi Iteration : 5 took: 9 seconds

---------------------
Model4: Iteration 6
10000
20000
#centers(pre/hillclimbed/real): 1 1 1  #al: 541.462 #alsophisticatedcountcollection: 3.11632 #hcsteps: 1.10322
#peggingImprovements: 0
D4 table contains 406 parameters.
A/D table contains 178377 parameters.
A/D table contains 172112 parameters.
NTable contains 108160 parameter.
p0_count is 442288 and p1 is 5363.48; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 1.61872 PERPLEXITY 3.07102
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 1.62087 PERPLEXITY 3.0756

Model4 Viterbi Iteration : 6 took: 10 seconds
H333444 Training Finished at: Fri Oct 15 16:51:28 2021


Entire Viterbi H333444 Training took: 38 seconds
==========================================================

Entire Training took: 118 seconds
Program Finished at: Fri Oct 15 16:51:28 2021

==========================================================
Executing: rm -f /home/jwang/github/part-ii-project/working/exp2/train/giza.or-co/or-co.A3.final.gz
Executing: gzip /home/jwang/github/part-ii-project/working/exp2/train/giza.or-co/or-co.A3.final
(2.1a) running snt2cooc co-or @ Fri Oct 15 16:51:28 BST 2021

Executing: mkdir -p /home/jwang/github/part-ii-project/working/exp2/train/giza.co-or
Executing: /home/jwang/github/mosesdecoder/tools/snt2cooc.out /home/jwang/github/part-ii-project/working/exp2/train/corpus/or.vcb /home/jwang/github/part-ii-project/working/exp2/train/corpus/co.vcb /home/jwang/github/part-ii-project/working/exp2/train/corpus/co-or-int-train.snt > /home/jwang/github/part-ii-project/working/exp2/train/giza.co-or/co-or.cooc
/home/jwang/github/mosesdecoder/tools/snt2cooc.out /home/jwang/github/part-ii-project/working/exp2/train/corpus/or.vcb /home/jwang/github/part-ii-project/working/exp2/train/corpus/co.vcb /home/jwang/github/part-ii-project/working/exp2/train/corpus/co-or-int-train.snt > /home/jwang/github/part-ii-project/working/exp2/train/giza.co-or/co-or.cooc
line 1000
line 2000
line 3000
line 4000
line 5000
line 6000
line 7000
line 8000
line 9000
line 10000
line 11000
line 12000
line 13000
line 14000
line 15000
line 16000
line 17000
line 18000
line 19000
line 20000
line 21000
line 22000
line 23000
line 24000
line 25000
line 26000
line 27000
line 28000
END.
(2.1b) running giza co-or @ Fri Oct 15 16:51:31 BST 2021
/home/jwang/github/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/jwang/github/part-ii-project/working/exp2/train/giza.co-or/co-or.cooc -c /home/jwang/github/part-ii-project/working/exp2/train/corpus/co-or-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/jwang/github/part-ii-project/working/exp2/train/giza.co-or/co-or -onlyaldumps 1 -p0 0.999 -s /home/jwang/github/part-ii-project/working/exp2/train/corpus/or.vcb -t /home/jwang/github/part-ii-project/working/exp2/train/corpus/co.vcb
Executing: /home/jwang/github/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/jwang/github/part-ii-project/working/exp2/train/giza.co-or/co-or.cooc -c /home/jwang/github/part-ii-project/working/exp2/train/corpus/co-or-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/jwang/github/part-ii-project/working/exp2/train/giza.co-or/co-or -onlyaldumps 1 -p0 0.999 -s /home/jwang/github/part-ii-project/working/exp2/train/corpus/or.vcb -t /home/jwang/github/part-ii-project/working/exp2/train/corpus/co.vcb
/home/jwang/github/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/jwang/github/part-ii-project/working/exp2/train/giza.co-or/co-or.cooc -c /home/jwang/github/part-ii-project/working/exp2/train/corpus/co-or-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/jwang/github/part-ii-project/working/exp2/train/giza.co-or/co-or -onlyaldumps 1 -p0 0.999 -s /home/jwang/github/part-ii-project/working/exp2/train/corpus/or.vcb -t /home/jwang/github/part-ii-project/working/exp2/train/corpus/co.vcb
Parameter 'coocurrencefile' changed from '' to '/home/jwang/github/part-ii-project/working/exp2/train/giza.co-or/co-or.cooc'
Parameter 'c' changed from '' to '/home/jwang/github/part-ii-project/working/exp2/train/corpus/co-or-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '2021-10-15.165131.jwang' to '/home/jwang/github/part-ii-project/working/exp2/train/giza.co-or/co-or'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/jwang/github/part-ii-project/working/exp2/train/corpus/or.vcb'
Parameter 't' changed from '' to '/home/jwang/github/part-ii-project/working/exp2/train/corpus/co.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2021-10-15.165131.jwang.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/jwang/github/part-ii-project/working/exp2/train/giza.co-or/co-or  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/jwang/github/part-ii-project/working/exp2/train/corpus/co-or-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/jwang/github/part-ii-project/working/exp2/train/corpus/or.vcb  (source vocabulary file name)
t = /home/jwang/github/part-ii-project/working/exp2/train/corpus/co.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2021-10-15.165131.jwang.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/jwang/github/part-ii-project/working/exp2/train/giza.co-or/co-or  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/jwang/github/part-ii-project/working/exp2/train/corpus/co-or-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/jwang/github/part-ii-project/working/exp2/train/corpus/or.vcb  (source vocabulary file name)
t = /home/jwang/github/part-ii-project/working/exp2/train/corpus/co.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/jwang/github/part-ii-project/working/exp2/train/corpus/or.vcb
Reading vocabulary file from:/home/jwang/github/part-ii-project/working/exp2/train/corpus/co.vcb
Source vocabulary list has 13999 unique tokens 
Target vocabulary list has 10816 unique tokens 
Calculating vocabulary frequencies from corpus /home/jwang/github/part-ii-project/working/exp2/train/corpus/co-or-int-train.snt
Reading more sentence pairs into memory ... 
Corpus fits in memory, corpus has: 28327 sentence pairs.
 Train total # sentence pairs (weighted): 28327
Size of source portion of the training corpus: 453015 tokens
Size of the target portion of the training corpus: 457110 tokens 
In source portion of the training corpus, only 13998 unique tokens appeared
In target portion of the training corpus, only 10814 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 457110/(481342-28327)== 1.00904
There are 1495439 1495439 entries in table
==========================================================
Model1 Training Started at: Fri Oct 15 16:51:31 2021

-----------
Model1: Iteration 1
Model1: (1) TRAIN CROSS-ENTROPY 13.604 PERPLEXITY 12450.8
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 17.9816 PERPLEXITY 258824
Model 1 Iteration: 1 took: 1 seconds
-----------
Model1: Iteration 2
Model1: (2) TRAIN CROSS-ENTROPY 6.70033 PERPLEXITY 103.992
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 8.63514 PERPLEXITY 397.592
Model 1 Iteration: 2 took: 1 seconds
-----------
Model1: Iteration 3
Model1: (3) TRAIN CROSS-ENTROPY 5.32151 PERPLEXITY 39.9885
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 6.32656 PERPLEXITY 80.2574
Model 1 Iteration: 3 took: 1 seconds
-----------
Model1: Iteration 4
Model1: (4) TRAIN CROSS-ENTROPY 4.84339 PERPLEXITY 28.7082
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 5.49598 PERPLEXITY 45.129
Model 1 Iteration: 4 took: 0 seconds
-----------
Model1: Iteration 5
Model1: (5) TRAIN CROSS-ENTROPY 4.6962 PERPLEXITY 25.9238
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 5.19891 PERPLEXITY 36.7305
Model 1 Iteration: 5 took: 1 seconds
Entire Model1 Training took: 4 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 0  #classes: 1
Read classes: #words: 0  #classes: 1

==========================================================
Hmm Training Started at: Fri Oct 15 16:51:35 2021

-----------
Hmm: Iteration 1
A/D table contains 172665 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 4.61738 PERPLEXITY 24.5454
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 5.069 PERPLEXITY 33.5677

Hmm Iteration: 1 took: 7 seconds

-----------
Hmm: Iteration 2
A/D table contains 172665 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 2.01423 PERPLEXITY 4.03966
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 2.06603 PERPLEXITY 4.18733

Hmm Iteration: 2 took: 6 seconds

-----------
Hmm: Iteration 3
A/D table contains 172665 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 1.49732 PERPLEXITY 2.82317
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 1.51928 PERPLEXITY 2.86647

Hmm Iteration: 3 took: 6 seconds

-----------
Hmm: Iteration 4
A/D table contains 172665 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 1.39496 PERPLEXITY 2.62981
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 1.408 PERPLEXITY 2.65369

Hmm Iteration: 4 took: 6 seconds

-----------
Hmm: Iteration 5
A/D table contains 172665 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 1.35618 PERPLEXITY 2.56006
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 1.36616 PERPLEXITY 2.57784

Hmm Iteration: 5 took: 7 seconds

Entire Hmm Training took: 32 seconds
==========================================================
Read classes: #words: 0  #classes: 1
Read classes: #words: 0  #classes: 1
Read classes: #words: 0  #classes: 1
Read classes: #words: 0  #classes: 1

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Fri Oct 15 16:52:07 2021


---------------------
THTo3: Iteration 1
10000
20000
#centers(pre/hillclimbed/real): 1 1 1  #al: 544.958 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 172665 parameters.
A/D table contains 178118 parameters.
NTable contains 139990 parameter.
p0_count is 434569 and p1 is 11270.7; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 1.14188 PERPLEXITY 2.20669
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 1.14905 PERPLEXITY 2.21768

THTo3 Viterbi Iteration : 1 took: 5 seconds

---------------------
Model3: Iteration 2
10000
20000
#centers(pre/hillclimbed/real): 1 1 1  #al: 545.022 #alsophisticatedcountcollection: 0 #hcsteps: 1.14389
#peggingImprovements: 0
A/D table contains 172665 parameters.
A/D table contains 178118 parameters.
NTable contains 139990 parameter.
p0_count is 438142 and p1 is 9484.19; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 1.97112 PERPLEXITY 3.92072
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 1.97666 PERPLEXITY 3.9358

Model3 Viterbi Iteration : 2 took: 4 seconds

---------------------
Model3: Iteration 3
10000
20000
#centers(pre/hillclimbed/real): 1 1 1  #al: 545.037 #alsophisticatedcountcollection: 0 #hcsteps: 1.15123
#peggingImprovements: 0
A/D table contains 172665 parameters.
A/D table contains 178118 parameters.
NTable contains 139990 parameter.
p0_count is 439170 and p1 is 8970.09; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 1.94655 PERPLEXITY 3.85451
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 1.95115 PERPLEXITY 3.86683

Model3 Viterbi Iteration : 3 took: 4 seconds

---------------------
T3To4: Iteration 4
10000
20000
#centers(pre/hillclimbed/real): 1 1 1  #al: 545.042 #alsophisticatedcountcollection: 6.21051 #hcsteps: 1.15702
#peggingImprovements: 0
D4 table contains 406 parameters.
A/D table contains 172665 parameters.
A/D table contains 178118 parameters.
NTable contains 139990 parameter.
p0_count is 439905 and p1 is 8602.57; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 1.93477 PERPLEXITY 3.82317
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 1.93885 PERPLEXITY 3.834

T3To4 Viterbi Iteration : 4 took: 5 seconds

---------------------
Model4: Iteration 5
10000
20000
#centers(pre/hillclimbed/real): 1 1 1  #al: 545.035 #alsophisticatedcountcollection: 3.13754 #hcsteps: 1.12486
#peggingImprovements: 0
D4 table contains 406 parameters.
A/D table contains 172665 parameters.
A/D table contains 178038 parameters.
NTable contains 139990 parameter.
p0_count is 439995 and p1 is 8557.31; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 1.63672 PERPLEXITY 3.10959
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 1.63933 PERPLEXITY 3.1152

Model4 Viterbi Iteration : 5 took: 9 seconds

---------------------
Model4: Iteration 6
10000
20000
#centers(pre/hillclimbed/real): 1 1 1  #al: 545.034 #alsophisticatedcountcollection: 3.03594 #hcsteps: 1.12843
#peggingImprovements: 0
D4 table contains 406 parameters.
A/D table contains 172665 parameters.
A/D table contains 178118 parameters.
NTable contains 139990 parameter.
p0_count is 440565 and p1 is 8272.62; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 1.61403 PERPLEXITY 3.06106
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 1.61647 PERPLEXITY 3.06624

Model4 Viterbi Iteration : 6 took: 10 seconds
H333444 Training Finished at: Fri Oct 15 16:52:44 2021


Entire Viterbi H333444 Training took: 37 seconds
==========================================================

Entire Training took: 73 seconds
Program Finished at: Fri Oct 15 16:52:44 2021

==========================================================
Executing: rm -f /home/jwang/github/part-ii-project/working/exp2/train/giza.co-or/co-or.A3.final.gz
Executing: gzip /home/jwang/github/part-ii-project/working/exp2/train/giza.co-or/co-or.A3.final
(3) generate word alignment @ Fri Oct 15 16:52:44 BST 2021
Combining forward and inverted alignment from files:
  /home/jwang/github/part-ii-project/working/exp2/train/giza.or-co/or-co.A3.final.{bz2,gz}
  /home/jwang/github/part-ii-project/working/exp2/train/giza.co-or/co-or.A3.final.{bz2,gz}
Executing: mkdir -p /home/jwang/github/part-ii-project/working/exp2/train/model
Executing: /home/jwang/github/mosesdecoder/scripts/training/giza2bal.pl -d "gzip -cd /home/jwang/github/part-ii-project/working/exp2/train/giza.co-or/co-or.A3.final.gz" -i "gzip -cd /home/jwang/github/part-ii-project/working/exp2/train/giza.or-co/or-co.A3.final.gz" |/home/jwang/github/mosesdecoder/scripts/../bin/symal -alignment="grow" -diagonal="yes" -final="yes" -both="yes" > /home/jwang/github/part-ii-project/working/exp2/train/model/aligned.grow-diag-final-and
symal: computing grow alignment: diagonal (1) final (1)both-uncovered (1)
skip=<0> counts=<28327>
(4) generate lexical translation table 0-0 @ Fri Oct 15 16:52:47 BST 2021
(/home/jwang/github/part-ii-project/corpus/training/fce.train.gold.bea19.clean.or,/home/jwang/github/part-ii-project/corpus/training/fce.train.gold.bea19.clean.co,/home/jwang/github/part-ii-project/working/exp2/train/model/lex)
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Saved: /home/jwang/github/part-ii-project/working/exp2/train/model/lex.f2e and /home/jwang/github/part-ii-project/working/exp2/train/model/lex.e2f
FILE: /home/jwang/github/part-ii-project/corpus/training/fce.train.gold.bea19.clean.co
FILE: /home/jwang/github/part-ii-project/corpus/training/fce.train.gold.bea19.clean.or
FILE: /home/jwang/github/part-ii-project/working/exp2/train/model/aligned.grow-diag-final-and
(5) extract phrases @ Fri Oct 15 16:52:48 BST 2021
/home/jwang/github/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /home/jwang/github/mosesdecoder/scripts/../bin/extract /home/jwang/github/part-ii-project/corpus/training/fce.train.gold.bea19.clean.co /home/jwang/github/part-ii-project/corpus/training/fce.train.gold.bea19.clean.or /home/jwang/github/part-ii-project/working/exp2/train/model/aligned.grow-diag-final-and /home/jwang/github/part-ii-project/working/exp2/train/model/extract 7 orientation --model phrase-msd --GZOutput 
Executing: /home/jwang/github/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /home/jwang/github/mosesdecoder/scripts/../bin/extract /home/jwang/github/part-ii-project/corpus/training/fce.train.gold.bea19.clean.co /home/jwang/github/part-ii-project/corpus/training/fce.train.gold.bea19.clean.or /home/jwang/github/part-ii-project/working/exp2/train/model/aligned.grow-diag-final-and /home/jwang/github/part-ii-project/working/exp2/train/model/extract 7 orientation --model phrase-msd --GZOutput 
MAX 7 1 0
Started Fri Oct 15 16:52:48 2021
using gzip 
isBSDSplit=0 
Executing: mkdir -p /home/jwang/github/part-ii-project/working/exp2/train/model/tmp.512; ls -l /home/jwang/github/part-ii-project/working/exp2/train/model/tmp.512 
total=28327 line-per-split=7082 
split -d -l 7082 -a 7 /home/jwang/github/part-ii-project/corpus/training/fce.train.gold.bea19.clean.co /home/jwang/github/part-ii-project/working/exp2/train/model/tmp.512/target.split -d -l 7082 -a 7 /home/jwang/github/part-ii-project/corpus/training/fce.train.gold.bea19.clean.or /home/jwang/github/part-ii-project/working/exp2/train/model/tmp.512/source.split -d -l 7082 -a 7 /home/jwang/github/part-ii-project/working/exp2/train/model/aligned.grow-diag-final-and /home/jwang/github/part-ii-project/working/exp2/train/model/tmp.512/align.merging extract / extract.inv
gunzip -c /home/jwang/github/part-ii-project/working/exp2/train/model/tmp.512/extract.0000000.gz /home/jwang/github/part-ii-project/working/exp2/train/model/tmp.512/extract.0000001.gz /home/jwang/github/part-ii-project/working/exp2/train/model/tmp.512/extract.0000002.gz /home/jwang/github/part-ii-project/working/exp2/train/model/tmp.512/extract.0000003.gz  | LC_ALL=C sort     -T /home/jwang/github/part-ii-project/working/exp2/train/model/tmp.512 2>> /dev/stderr | gzip -c > /home/jwang/github/part-ii-project/working/exp2/train/model/extract.sorted.gz 2>> /dev/stderr 
gunzip -c /home/jwang/github/part-ii-project/working/exp2/train/model/tmp.512/extract.0000000.inv.gz /home/jwang/github/part-ii-project/working/exp2/train/model/tmp.512/extract.0000001.inv.gz /home/jwang/github/part-ii-project/working/exp2/train/model/tmp.512/extract.0000002.inv.gz /home/jwang/github/part-ii-project/working/exp2/train/model/tmp.512/extract.0000003.inv.gz  | LC_ALL=C sort     -T /home/jwang/github/part-ii-project/working/exp2/train/model/tmp.512 2>> /dev/stderr | gzip -c > /home/jwang/github/part-ii-project/working/exp2/train/model/extract.inv.sorted.gz 2>> /dev/stderr 
gunzip -c /home/jwang/github/part-ii-project/working/exp2/train/model/tmp.512/extract.0000000.o.gz /home/jwang/github/part-ii-project/working/exp2/train/model/tmp.512/extract.0000001.o.gz /home/jwang/github/part-ii-project/working/exp2/train/model/tmp.512/extract.0000002.o.gz /home/jwang/github/part-ii-project/working/exp2/train/model/tmp.512/extract.0000003.o.gz  | LC_ALL=C sort     -T /home/jwang/github/part-ii-project/working/exp2/train/model/tmp.512 2>> /dev/stderr | gzip -c > /home/jwang/github/part-ii-project/working/exp2/train/model/extract.o.sorted.gz 2>> /dev/stderr 
Finished Fri Oct 15 16:53:02 2021
(6) score phrases @ Fri Oct 15 16:53:03 BST 2021
(6.1)  creating table half /home/jwang/github/part-ii-project/working/exp2/train/model/phrase-table.half.f2e @ Fri Oct 15 16:53:03 BST 2021
/home/jwang/github/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/jwang/github/mosesdecoder/scripts/../bin/score /home/jwang/github/part-ii-project/working/exp2/train/model/extract.sorted.gz /home/jwang/github/part-ii-project/working/exp2/train/model/lex.f2e /home/jwang/github/part-ii-project/working/exp2/train/model/phrase-table.half.f2e.gz  0 
Executing: /home/jwang/github/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/jwang/github/mosesdecoder/scripts/../bin/score /home/jwang/github/part-ii-project/working/exp2/train/model/extract.sorted.gz /home/jwang/github/part-ii-project/working/exp2/train/model/lex.f2e /home/jwang/github/part-ii-project/working/exp2/train/model/phrase-table.half.f2e.gz  0 
using gzip 
Started Fri Oct 15 16:53:03 2021
/home/jwang/github/mosesdecoder/scripts/../bin/score /home/jwang/github/part-ii-project/working/exp2/train/model/tmp.561/extract.0.gz /home/jwang/github/part-ii-project/working/exp2/train/model/lex.f2e /home/jwang/github/part-ii-project/working/exp2/train/model/tmp.561/phrase-table.half.0000000.gz  2>> /dev/stderr 
/home/jwang/github/part-ii-project/working/exp2/train/model/tmp.561/run.0.sh/home/jwang/github/part-ii-project/working/exp2/train/model/tmp.561/run.1.sh/home/jwang/github/part-ii-project/working/exp2/train/model/tmp.561/run.2.sh/home/jwang/github/part-ii-project/working/exp2/train/model/tmp.561/run.3.shmv /home/jwang/github/part-ii-project/working/exp2/train/model/tmp.561/phrase-table.half.0000000.gz /home/jwang/github/part-ii-project/working/exp2/train/model/phrase-table.half.f2e.gzrm -rf /home/jwang/github/part-ii-project/working/exp2/train/model/tmp.561 
Finished Fri Oct 15 16:53:20 2021
(6.3)  creating table half /home/jwang/github/part-ii-project/working/exp2/train/model/phrase-table.half.e2f @ Fri Oct 15 16:53:20 BST 2021
/home/jwang/github/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/jwang/github/mosesdecoder/scripts/../bin/score /home/jwang/github/part-ii-project/working/exp2/train/model/extract.inv.sorted.gz /home/jwang/github/part-ii-project/working/exp2/train/model/lex.e2f /home/jwang/github/part-ii-project/working/exp2/train/model/phrase-table.half.e2f.gz --Inverse 1 
Executing: /home/jwang/github/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/jwang/github/mosesdecoder/scripts/../bin/score /home/jwang/github/part-ii-project/working/exp2/train/model/extract.inv.sorted.gz /home/jwang/github/part-ii-project/working/exp2/train/model/lex.e2f /home/jwang/github/part-ii-project/working/exp2/train/model/phrase-table.half.e2f.gz --Inverse 1 
using gzip 
Started Fri Oct 15 16:53:20 2021
/home/jwang/github/mosesdecoder/scripts/../bin/score /home/jwang/github/part-ii-project/working/exp2/train/model/tmp.584/extract.0.gz /home/jwang/github/part-ii-project/working/exp2/train/model/lex.e2f /home/jwang/github/part-ii-project/working/exp2/train/model/tmp.584/phrase-table.half.0000000.gz --Inverse  2>> /dev/stderr 
/home/jwang/github/part-ii-project/working/exp2/train/model/tmp.584/run.0.sh/home/jwang/github/part-ii-project/working/exp2/train/model/tmp.584/run.1.sh/home/jwang/github/part-ii-project/working/exp2/train/model/tmp.584/run.2.sh/home/jwang/github/part-ii-project/working/exp2/train/model/tmp.584/run.3.shgunzip -c /home/jwang/github/part-ii-project/working/exp2/train/model/tmp.584/phrase-table.half.*.gz 2>> /dev/stderr| LC_ALL=C sort     -T /home/jwang/github/part-ii-project/working/exp2/train/model/tmp.584  | gzip -c > /home/jwang/github/part-ii-project/working/exp2/train/model/phrase-table.half.e2f.gz  2>> /dev/stderr rm -rf /home/jwang/github/part-ii-project/working/exp2/train/model/tmp.584 
Finished Fri Oct 15 16:53:43 2021
(6.6) consolidating the two halves @ Fri Oct 15 16:53:43 BST 2021
Executing: /home/jwang/github/mosesdecoder/scripts/../bin/consolidate /home/jwang/github/part-ii-project/working/exp2/train/model/phrase-table.half.f2e.gz /home/jwang/github/part-ii-project/working/exp2/train/model/phrase-table.half.e2f.gz /dev/stdout | gzip -c > /home/jwang/github/part-ii-project/working/exp2/train/model/phrase-table.gz
Consolidate v2.0 written by Philipp Koehn
consolidating direct and indirect rule tables
...............
Executing: rm -f /home/jwang/github/part-ii-project/working/exp2/train/model/phrase-table.half.*
(7) learn reordering model @ Fri Oct 15 16:53:50 BST 2021
(7.1) [no factors] learn reordering model @ Fri Oct 15 16:53:50 BST 2021
(7.2) building tables @ Fri Oct 15 16:53:50 BST 2021
Executing: /home/jwang/github/mosesdecoder/scripts/../bin/lexical-reordering-score /home/jwang/github/part-ii-project/working/exp2/train/model/extract.o.sorted.gz 0.5 /home/jwang/github/part-ii-project/working/exp2/train/model/reordering-table. --model "phrase msd phrase-msd-bidirectional-fe"
Lexical Reordering Scorer
scores lexical reordering models of several types (hierarchical, phrase-based and word-based-extraction
(8) learn generation model @ Fri Oct 15 16:53:56 BST 2021
  no generation model requested, skipping step
(9) create moses.ini @ Fri Oct 15 16:53:56 BST 2021
