Using SCRIPTS_ROOTDIR: /home/jwang/github/mosesdecoder/scripts
Using single-thread GIZA
using gzip 
(1) preparing corpus @ Wed Oct 13 20:36:23 BST 2021
Executing: mkdir -p /home/jwang/github/part-ii-project/working/exp1/train/corpus
(1.0) selecting factors @ Wed Oct 13 20:36:23 BST 2021
(1.1) running mkcls  @ Wed Oct 13 20:36:24 BST 2021
/home/jwang/github/mosesdecoder/tools/mkcls -c50 -n2 -p/home/jwang/corpus/training/fce.train.gold.bea19.clean.or -V/home/jwang/github/part-ii-project/working/exp1/train/corpus/or.vcb.classes opt
Executing: /home/jwang/github/mosesdecoder/tools/mkcls -c50 -n2 -p/home/jwang/corpus/training/fce.train.gold.bea19.clean.or -V/home/jwang/github/part-ii-project/working/exp1/train/corpus/or.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 13437

start-costs: MEAN: 6.25516e+06 (6.24991e+06-6.26041e+06)  SIGMA:5250.33   
  end-costs: MEAN: 5.79539e+06 (5.79331e+06-5.79747e+06)  SIGMA:2082.92   
   start-pp: MEAN: 270.303 (267.406-273.2)  SIGMA:2.89699   
     end-pp: MEAN: 105.734 (105.285-106.184)  SIGMA:0.449584   
 iterations: MEAN: 320954 (315159-326749)  SIGMA:5795   
       time: MEAN: 9.59375 (7.3125-11.875)  SIGMA:2.28125   
(1.1) running mkcls  @ Wed Oct 13 20:36:44 BST 2021
/home/jwang/github/mosesdecoder/tools/mkcls -c50 -n2 -p/home/jwang/corpus/training/fce.train.gold.bea19.clean.co -V/home/jwang/github/part-ii-project/working/exp1/train/corpus/co.vcb.classes opt
Executing: /home/jwang/github/mosesdecoder/tools/mkcls -c50 -n2 -p/home/jwang/corpus/training/fce.train.gold.bea19.clean.co -V/home/jwang/github/part-ii-project/working/exp1/train/corpus/co.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 10299

start-costs: MEAN: 6.30138e+06 (6.30026e+06-6.30249e+06)  SIGMA:1113.2   
  end-costs: MEAN: 5.82964e+06 (5.8279e+06-5.83139e+06)  SIGMA:1745.93   
   start-pp: MEAN: 249.165 (248.604-249.727)  SIGMA:0.561692   
     end-pp: MEAN: 95.8544 (95.5155-96.1933)  SIGMA:0.338904   
 iterations: MEAN: 273982 (270023-277940)  SIGMA:3958.5   
       time: MEAN: 8.47656 (6.20312-10.75)  SIGMA:2.27344   
(1.2) creating vcb file /home/jwang/github/part-ii-project/working/exp1/train/corpus/or.vcb @ Wed Oct 13 20:37:01 BST 2021
(1.2) creating vcb file /home/jwang/github/part-ii-project/working/exp1/train/corpus/co.vcb @ Wed Oct 13 20:37:02 BST 2021
(1.3) numberizing corpus /home/jwang/github/part-ii-project/working/exp1/train/corpus/or-co-int-train.snt @ Wed Oct 13 20:37:02 BST 2021
(1.3) numberizing corpus /home/jwang/github/part-ii-project/working/exp1/train/corpus/co-or-int-train.snt @ Wed Oct 13 20:37:03 BST 2021
(2) running giza @ Wed Oct 13 20:37:04 BST 2021
(2.1a) running snt2cooc or-co @ Wed Oct 13 20:37:04 BST 2021

Executing: mkdir -p /home/jwang/github/part-ii-project/working/exp1/train/giza.or-co
Executing: /home/jwang/github/mosesdecoder/tools/snt2cooc.out /home/jwang/github/part-ii-project/working/exp1/train/corpus/co.vcb /home/jwang/github/part-ii-project/working/exp1/train/corpus/or.vcb /home/jwang/github/part-ii-project/working/exp1/train/corpus/or-co-int-train.snt > /home/jwang/github/part-ii-project/working/exp1/train/giza.or-co/or-co.cooc
/home/jwang/github/mosesdecoder/tools/snt2cooc.out /home/jwang/github/part-ii-project/working/exp1/train/corpus/co.vcb /home/jwang/github/part-ii-project/working/exp1/train/corpus/or.vcb /home/jwang/github/part-ii-project/working/exp1/train/corpus/or-co-int-train.snt > /home/jwang/github/part-ii-project/working/exp1/train/giza.or-co/or-co.cooc
line 1000
line 2000
line 3000
line 4000
line 5000
line 6000
line 7000
line 8000
line 9000
line 10000
line 11000
line 12000
line 13000
line 14000
line 15000
line 16000
line 17000
line 18000
line 19000
line 20000
line 21000
line 22000
line 23000
line 24000
line 25000
line 26000
line 27000
line 28000
END.
(2.1b) running giza or-co @ Wed Oct 13 20:37:07 BST 2021
/home/jwang/github/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/jwang/github/part-ii-project/working/exp1/train/giza.or-co/or-co.cooc -c /home/jwang/github/part-ii-project/working/exp1/train/corpus/or-co-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/jwang/github/part-ii-project/working/exp1/train/giza.or-co/or-co -onlyaldumps 1 -p0 0.999 -s /home/jwang/github/part-ii-project/working/exp1/train/corpus/co.vcb -t /home/jwang/github/part-ii-project/working/exp1/train/corpus/or.vcb
Executing: /home/jwang/github/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/jwang/github/part-ii-project/working/exp1/train/giza.or-co/or-co.cooc -c /home/jwang/github/part-ii-project/working/exp1/train/corpus/or-co-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/jwang/github/part-ii-project/working/exp1/train/giza.or-co/or-co -onlyaldumps 1 -p0 0.999 -s /home/jwang/github/part-ii-project/working/exp1/train/corpus/co.vcb -t /home/jwang/github/part-ii-project/working/exp1/train/corpus/or.vcb
/home/jwang/github/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/jwang/github/part-ii-project/working/exp1/train/giza.or-co/or-co.cooc -c /home/jwang/github/part-ii-project/working/exp1/train/corpus/or-co-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/jwang/github/part-ii-project/working/exp1/train/giza.or-co/or-co -onlyaldumps 1 -p0 0.999 -s /home/jwang/github/part-ii-project/working/exp1/train/corpus/co.vcb -t /home/jwang/github/part-ii-project/working/exp1/train/corpus/or.vcb
Parameter 'coocurrencefile' changed from '' to '/home/jwang/github/part-ii-project/working/exp1/train/giza.or-co/or-co.cooc'
Parameter 'c' changed from '' to '/home/jwang/github/part-ii-project/working/exp1/train/corpus/or-co-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '2021-10-13.203707.jwang' to '/home/jwang/github/part-ii-project/working/exp1/train/giza.or-co/or-co'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/jwang/github/part-ii-project/working/exp1/train/corpus/co.vcb'
Parameter 't' changed from '' to '/home/jwang/github/part-ii-project/working/exp1/train/corpus/or.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2021-10-13.203707.jwang.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/jwang/github/part-ii-project/working/exp1/train/giza.or-co/or-co  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/jwang/github/part-ii-project/working/exp1/train/corpus/or-co-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/jwang/github/part-ii-project/working/exp1/train/corpus/co.vcb  (source vocabulary file name)
t = /home/jwang/github/part-ii-project/working/exp1/train/corpus/or.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2021-10-13.203707.jwang.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/jwang/github/part-ii-project/working/exp1/train/giza.or-co/or-co  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/jwang/github/part-ii-project/working/exp1/train/corpus/or-co-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/jwang/github/part-ii-project/working/exp1/train/corpus/co.vcb  (source vocabulary file name)
t = /home/jwang/github/part-ii-project/working/exp1/train/corpus/or.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/jwang/github/part-ii-project/working/exp1/train/corpus/co.vcb
Reading vocabulary file from:/home/jwang/github/part-ii-project/working/exp1/train/corpus/or.vcb
Source vocabulary list has 10300 unique tokens 
Target vocabulary list has 13438 unique tokens 
Calculating vocabulary frequencies from corpus /home/jwang/github/part-ii-project/working/exp1/train/corpus/or-co-int-train.snt
Reading more sentence pairs into memory ... 
{WARNING:(a)truncated sentence 5626}{WARNING:(b)truncated sentence 5626}{WARNING:(a)truncated sentence 16557}{WARNING:(b)truncated sentence 16557}{WARNING:(a)truncated sentence 24299}{WARNING:(b)truncated sentence 24299}Corpus fits in memory, corpus has: 28345 sentence pairs.
 Train total # sentence pairs (weighted): 28345
Size of source portion of the training corpus: 465454 tokens
Size of the target portion of the training corpus: 461507 tokens 
In source portion of the training corpus, only 10299 unique tokens appeared
In target portion of the training corpus, only 13436 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 461507/(493799-28345)== 0.99152
There are 1428394 1428394 entries in table
==========================================================
Model1 Training Started at: Wed Oct 13 20:37:08 2021

-----------
Model1: Iteration 1
Model1: (1) TRAIN CROSS-ENTROPY 13.9181 PERPLEXITY 15479.4
Model1: (1) VITERBI TRAIN CROSS-ENTROPY inf PERPLEXITY inf
Model 1 Iteration: 1 took: 1 seconds
-----------
Model1: Iteration 2
Model1: (2) TRAIN CROSS-ENTROPY 6.78986 PERPLEXITY 110.65
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 8.76375 PERPLEXITY 434.663
Model 1 Iteration: 2 took: 1 seconds
-----------
Model1: Iteration 3
Model1: (3) TRAIN CROSS-ENTROPY 5.41439 PERPLEXITY 42.6476
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 6.44883 PERPLEXITY 87.3559
Model 1 Iteration: 3 took: 2 seconds
-----------
Model1: Iteration 4
Model1: (4) TRAIN CROSS-ENTROPY 4.93065 PERPLEXITY 30.4981
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 5.60783 PERPLEXITY 48.7668
Model 1 Iteration: 4 took: 2 seconds
-----------
Model1: Iteration 5
Model1: (5) TRAIN CROSS-ENTROPY 4.77898 PERPLEXITY 27.4547
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 5.30361 PERPLEXITY 39.4954
Model 1 Iteration: 5 took: 2 seconds
Entire Model1 Training took: 8 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 10299  #classes: 51
Read classes: #words: 13437  #classes: 51

==========================================================
Hmm Training Started at: Wed Oct 13 20:37:17 2021

-----------
Hmm: Iteration 1
A/D table contains 258687 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 4.68674 PERPLEXITY 25.7542
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 5.17047 PERPLEXITY 36.0137

Hmm Iteration: 1 took: 21 seconds

-----------
Hmm: Iteration 2
A/D table contains 258687 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 2.10244 PERPLEXITY 4.29435
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 2.16206 PERPLEXITY 4.47554

Hmm Iteration: 2 took: 21 seconds

-----------
Hmm: Iteration 3
A/D table contains 258687 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 1.56025 PERPLEXITY 2.94906
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 1.58305 PERPLEXITY 2.99602

Hmm Iteration: 3 took: 21 seconds

-----------
Hmm: Iteration 4
A/D table contains 258687 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 1.45399 PERPLEXITY 2.73964
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 1.46659 PERPLEXITY 2.76367

Hmm Iteration: 4 took: 20 seconds

-----------
Hmm: Iteration 5
A/D table contains 258687 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 1.41641 PERPLEXITY 2.66921
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 1.42658 PERPLEXITY 2.68808

Hmm Iteration: 5 took: 23 seconds

Entire Hmm Training took: 106 seconds
==========================================================
Read classes: #words: 10299  #classes: 51
Read classes: #words: 13437  #classes: 51
Read classes: #words: 10299  #classes: 51
Read classes: #words: 13437  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Wed Oct 13 20:39:03 2021


---------------------
THTo3: Iteration 1
10000
20000
#centers(pre/hillclimbed/real): 1 1 1  #al: 565.182 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 258687 parameters.
A/D table contains 273121 parameters.
NTable contains 103000 parameter.
p0_count is 444515 and p1 is 8495.76; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 1.20203 PERPLEXITY 2.30064
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 1.20954 PERPLEXITY 2.31264

THTo3 Viterbi Iteration : 1 took: 16 seconds

---------------------
Model3: Iteration 2
10000
20000
#centers(pre/hillclimbed/real): 1 1 1  #al: 565.241 #alsophisticatedcountcollection: 0 #hcsteps: 1.14105
#peggingImprovements: 0
A/D table contains 258687 parameters.
A/D table contains 273121 parameters.
NTable contains 103000 parameter.
p0_count is 448327 and p1 is 6589.78; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 2.00545 PERPLEXITY 4.01514
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 2.01079 PERPLEXITY 4.03004

Model3 Viterbi Iteration : 2 took: 14 seconds

---------------------
Model3: Iteration 3
10000
20000
#centers(pre/hillclimbed/real): 1 1 1  #al: 565.255 #alsophisticatedcountcollection: 0 #hcsteps: 1.14122
#peggingImprovements: 0
A/D table contains 258687 parameters.
A/D table contains 273121 parameters.
NTable contains 103000 parameter.
p0_count is 449172 and p1 is 6167.42; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 1.98367 PERPLEXITY 3.95498
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 1.98825 PERPLEXITY 3.96754

Model3 Viterbi Iteration : 3 took: 14 seconds

---------------------
T3To4: Iteration 4
10000
20000
#centers(pre/hillclimbed/real): 1 1 1  #al: 565.262 #alsophisticatedcountcollection: 6.77005 #hcsteps: 1.1462
#peggingImprovements: 0
D4 table contains 506485 parameters.
A/D table contains 258687 parameters.
A/D table contains 273040 parameters.
NTable contains 103000 parameter.
p0_count is 449771 and p1 is 5867.91; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 1.97429 PERPLEXITY 3.92934
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 1.97851 PERPLEXITY 3.94085

T3To4 Viterbi Iteration : 4 took: 15 seconds

---------------------
Model4: Iteration 5
10000
20000
#centers(pre/hillclimbed/real): 1 1 1  #al: 565.258 #alsophisticatedcountcollection: 4.09804 #hcsteps: 1.11402
#peggingImprovements: 0
D4 table contains 506485 parameters.
A/D table contains 258687 parameters.
A/D table contains 273342 parameters.
NTable contains 103000 parameter.
p0_count is 450063 and p1 is 5722.09; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 1.61173 PERPLEXITY 3.05618
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 1.61399 PERPLEXITY 3.06097

Model4 Viterbi Iteration : 5 took: 34 seconds

---------------------
Model4: Iteration 6
10000
20000
#centers(pre/hillclimbed/real): 1 1 1  #al: 565.256 #alsophisticatedcountcollection: 3.85885 #hcsteps: 1.11304
#peggingImprovements: 0
D4 table contains 506485 parameters.
A/D table contains 258687 parameters.
A/D table contains 273342 parameters.
NTable contains 103000 parameter.
p0_count is 450348 and p1 is 5579.73; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 1.59366 PERPLEXITY 3.01815
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 1.59571 PERPLEXITY 3.02243

Model4 Viterbi Iteration : 6 took: 35 seconds
H333444 Training Finished at: Wed Oct 13 20:41:11 2021


Entire Viterbi H333444 Training took: 128 seconds
==========================================================

Entire Training took: 244 seconds
Program Finished at: Wed Oct 13 20:41:11 2021

==========================================================
Executing: rm -f /home/jwang/github/part-ii-project/working/exp1/train/giza.or-co/or-co.A3.final.gz
Executing: gzip /home/jwang/github/part-ii-project/working/exp1/train/giza.or-co/or-co.A3.final
(2.1a) running snt2cooc co-or @ Wed Oct 13 20:41:12 BST 2021

Executing: mkdir -p /home/jwang/github/part-ii-project/working/exp1/train/giza.co-or
Executing: /home/jwang/github/mosesdecoder/tools/snt2cooc.out /home/jwang/github/part-ii-project/working/exp1/train/corpus/or.vcb /home/jwang/github/part-ii-project/working/exp1/train/corpus/co.vcb /home/jwang/github/part-ii-project/working/exp1/train/corpus/co-or-int-train.snt > /home/jwang/github/part-ii-project/working/exp1/train/giza.co-or/co-or.cooc
/home/jwang/github/mosesdecoder/tools/snt2cooc.out /home/jwang/github/part-ii-project/working/exp1/train/corpus/or.vcb /home/jwang/github/part-ii-project/working/exp1/train/corpus/co.vcb /home/jwang/github/part-ii-project/working/exp1/train/corpus/co-or-int-train.snt > /home/jwang/github/part-ii-project/working/exp1/train/giza.co-or/co-or.cooc
line 1000
line 2000
line 3000
line 4000
line 5000
line 6000
line 7000
line 8000
line 9000
line 10000
line 11000
line 12000
line 13000
line 14000
line 15000
line 16000
line 17000
line 18000
line 19000
line 20000
line 21000
line 22000
line 23000
line 24000
line 25000
line 26000
line 27000
line 28000
END.
(2.1b) running giza co-or @ Wed Oct 13 20:41:15 BST 2021
/home/jwang/github/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/jwang/github/part-ii-project/working/exp1/train/giza.co-or/co-or.cooc -c /home/jwang/github/part-ii-project/working/exp1/train/corpus/co-or-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/jwang/github/part-ii-project/working/exp1/train/giza.co-or/co-or -onlyaldumps 1 -p0 0.999 -s /home/jwang/github/part-ii-project/working/exp1/train/corpus/or.vcb -t /home/jwang/github/part-ii-project/working/exp1/train/corpus/co.vcb
Executing: /home/jwang/github/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/jwang/github/part-ii-project/working/exp1/train/giza.co-or/co-or.cooc -c /home/jwang/github/part-ii-project/working/exp1/train/corpus/co-or-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/jwang/github/part-ii-project/working/exp1/train/giza.co-or/co-or -onlyaldumps 1 -p0 0.999 -s /home/jwang/github/part-ii-project/working/exp1/train/corpus/or.vcb -t /home/jwang/github/part-ii-project/working/exp1/train/corpus/co.vcb
/home/jwang/github/mosesdecoder/tools/GIZA++  -CoocurrenceFile /home/jwang/github/part-ii-project/working/exp1/train/giza.co-or/co-or.cooc -c /home/jwang/github/part-ii-project/working/exp1/train/corpus/co-or-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/jwang/github/part-ii-project/working/exp1/train/giza.co-or/co-or -onlyaldumps 1 -p0 0.999 -s /home/jwang/github/part-ii-project/working/exp1/train/corpus/or.vcb -t /home/jwang/github/part-ii-project/working/exp1/train/corpus/co.vcb
Parameter 'coocurrencefile' changed from '' to '/home/jwang/github/part-ii-project/working/exp1/train/giza.co-or/co-or.cooc'
Parameter 'c' changed from '' to '/home/jwang/github/part-ii-project/working/exp1/train/corpus/co-or-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '2021-10-13.204115.jwang' to '/home/jwang/github/part-ii-project/working/exp1/train/giza.co-or/co-or'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/jwang/github/part-ii-project/working/exp1/train/corpus/or.vcb'
Parameter 't' changed from '' to '/home/jwang/github/part-ii-project/working/exp1/train/corpus/co.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2021-10-13.204115.jwang.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/jwang/github/part-ii-project/working/exp1/train/giza.co-or/co-or  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/jwang/github/part-ii-project/working/exp1/train/corpus/co-or-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/jwang/github/part-ii-project/working/exp1/train/corpus/or.vcb  (source vocabulary file name)
t = /home/jwang/github/part-ii-project/working/exp1/train/corpus/co.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2021-10-13.204115.jwang.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/jwang/github/part-ii-project/working/exp1/train/giza.co-or/co-or  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/jwang/github/part-ii-project/working/exp1/train/corpus/co-or-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/jwang/github/part-ii-project/working/exp1/train/corpus/or.vcb  (source vocabulary file name)
t = /home/jwang/github/part-ii-project/working/exp1/train/corpus/co.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/jwang/github/part-ii-project/working/exp1/train/corpus/or.vcb
Reading vocabulary file from:/home/jwang/github/part-ii-project/working/exp1/train/corpus/co.vcb
Source vocabulary list has 13438 unique tokens 
Target vocabulary list has 10300 unique tokens 
Calculating vocabulary frequencies from corpus /home/jwang/github/part-ii-project/working/exp1/train/corpus/co-or-int-train.snt
Reading more sentence pairs into memory ... 
{WARNING:(a)truncated sentence 5626}{WARNING:(b)truncated sentence 5626}{WARNING:(a)truncated sentence 16557}{WARNING:(b)truncated sentence 16557}{WARNING:(a)truncated sentence 24299}{WARNING:(b)truncated sentence 24299}Corpus fits in memory, corpus has: 28345 sentence pairs.
 Train total # sentence pairs (weighted): 28345
Size of source portion of the training corpus: 461507 tokens
Size of the target portion of the training corpus: 465454 tokens 
In source portion of the training corpus, only 13437 unique tokens appeared
In target portion of the training corpus, only 10298 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 465454/(489852-28345)== 1.00855
There are 1425256 1425256 entries in table
==========================================================
Model1 Training Started at: Wed Oct 13 20:41:15 2021

-----------
Model1: Iteration 1
Model1: (1) TRAIN CROSS-ENTROPY 13.5308 PERPLEXITY 11835.4
Model1: (1) VITERBI TRAIN CROSS-ENTROPY inf PERPLEXITY inf
Model 1 Iteration: 1 took: 1 seconds
-----------
Model1: Iteration 2
Model1: (2) TRAIN CROSS-ENTROPY 6.67093 PERPLEXITY 101.894
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 8.66247 PERPLEXITY 405.193
Model 1 Iteration: 2 took: 1 seconds
-----------
Model1: Iteration 3
Model1: (3) TRAIN CROSS-ENTROPY 5.33004 PERPLEXITY 40.2255
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 6.38172 PERPLEXITY 83.3853
Model 1 Iteration: 3 took: 2 seconds
-----------
Model1: Iteration 4
Model1: (4) TRAIN CROSS-ENTROPY 4.85246 PERPLEXITY 28.8892
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 5.54124 PERPLEXITY 46.5672
Model 1 Iteration: 4 took: 2 seconds
-----------
Model1: Iteration 5
Model1: (5) TRAIN CROSS-ENTROPY 4.70163 PERPLEXITY 26.0214
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 5.23484 PERPLEXITY 37.6569
Model 1 Iteration: 5 took: 2 seconds
Entire Model1 Training took: 8 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 13437  #classes: 51
Read classes: #words: 10299  #classes: 51

==========================================================
Hmm Training Started at: Wed Oct 13 20:41:23 2021

-----------
Hmm: Iteration 1
A/D table contains 274388 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 4.60718 PERPLEXITY 24.3724
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 5.10009 PERPLEXITY 34.2988

Hmm Iteration: 1 took: 21 seconds

-----------
Hmm: Iteration 2
A/D table contains 274388 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 2.03445 PERPLEXITY 4.09668
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 2.09382 PERPLEXITY 4.26877

Hmm Iteration: 2 took: 21 seconds

-----------
Hmm: Iteration 3
A/D table contains 274388 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 1.48458 PERPLEXITY 2.79835
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 1.50801 PERPLEXITY 2.84418

Hmm Iteration: 3 took: 20 seconds

-----------
Hmm: Iteration 4
A/D table contains 274388 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 1.37898 PERPLEXITY 2.60084
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 1.39229 PERPLEXITY 2.62496

Hmm Iteration: 4 took: 20 seconds

-----------
Hmm: Iteration 5
A/D table contains 274388 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 1.34125 PERPLEXITY 2.5337
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 1.35153 PERPLEXITY 2.55182

Hmm Iteration: 5 took: 21 seconds

Entire Hmm Training took: 103 seconds
==========================================================
Read classes: #words: 13437  #classes: 51
Read classes: #words: 10299  #classes: 51
Read classes: #words: 13437  #classes: 51
Read classes: #words: 10299  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Wed Oct 13 20:43:06 2021


---------------------
THTo3: Iteration 1
10000
20000
#centers(pre/hillclimbed/real): 1 1 1  #al: 568.716 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 274388 parameters.
A/D table contains 257916 parameters.
NTable contains 134380 parameter.
p0_count is 442588 and p1 is 11432.9; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 1.13046 PERPLEXITY 2.18928
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 1.1378 PERPLEXITY 2.20045

THTo3 Viterbi Iteration : 1 took: 16 seconds

---------------------
Model3: Iteration 2
10000
20000
#centers(pre/hillclimbed/real): 1 1 1  #al: 568.782 #alsophisticatedcountcollection: 0 #hcsteps: 1.15682
#peggingImprovements: 0
A/D table contains 274388 parameters.
A/D table contains 257916 parameters.
NTable contains 134380 parameter.
p0_count is 446369 and p1 is 9542.74; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 1.97549 PERPLEXITY 3.93262
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 1.98135 PERPLEXITY 3.94863

Model3 Viterbi Iteration : 2 took: 13 seconds

---------------------
Model3: Iteration 3
10000
20000
#centers(pre/hillclimbed/real): 1 1 1  #al: 568.795 #alsophisticatedcountcollection: 0 #hcsteps: 1.16278
#peggingImprovements: 0
A/D table contains 274388 parameters.
A/D table contains 257916 parameters.
NTable contains 134380 parameter.
p0_count is 447379 and p1 is 9037.65; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 1.95124 PERPLEXITY 3.86708
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 1.95605 PERPLEXITY 3.87998

Model3 Viterbi Iteration : 3 took: 14 seconds

---------------------
T3To4: Iteration 4
10000
20000
#centers(pre/hillclimbed/real): 1 1 1  #al: 568.802 #alsophisticatedcountcollection: 6.56687 #hcsteps: 1.16839
#peggingImprovements: 0
D4 table contains 507500 parameters.
A/D table contains 274388 parameters.
A/D table contains 257916 parameters.
NTable contains 134380 parameter.
p0_count is 448133 and p1 is 8660.71; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 1.93977 PERPLEXITY 3.83645
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 1.94399 PERPLEXITY 3.84769

T3To4 Viterbi Iteration : 4 took: 15 seconds

---------------------
Model4: Iteration 5
10000
20000
#centers(pre/hillclimbed/real): 1 1 1  #al: 568.798 #alsophisticatedcountcollection: 4.22674 #hcsteps: 1.13371
#peggingImprovements: 0
D4 table contains 507500 parameters.
A/D table contains 274388 parameters.
A/D table contains 258127 parameters.
NTable contains 134380 parameter.
p0_count is 448177 and p1 is 8638.74; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 1.5871 PERPLEXITY 3.00444
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 1.58968 PERPLEXITY 3.00982

Model4 Viterbi Iteration : 5 took: 34 seconds

---------------------
Model4: Iteration 6
10000
20000
#centers(pre/hillclimbed/real): 1 1 1  #al: 568.798 #alsophisticatedcountcollection: 4.07275 #hcsteps: 1.13382
#peggingImprovements: 0
D4 table contains 507500 parameters.
A/D table contains 274388 parameters.
A/D table contains 258127 parameters.
NTable contains 134380 parameter.
p0_count is 448561 and p1 is 8446.28; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 1.56853 PERPLEXITY 2.96603
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 1.57088 PERPLEXITY 2.97085

Model4 Viterbi Iteration : 6 took: 36 seconds
H333444 Training Finished at: Wed Oct 13 20:45:14 2021


Entire Viterbi H333444 Training took: 128 seconds
==========================================================

Entire Training took: 239 seconds
Program Finished at: Wed Oct 13 20:45:14 2021

==========================================================
Executing: rm -f /home/jwang/github/part-ii-project/working/exp1/train/giza.co-or/co-or.A3.final.gz
Executing: gzip /home/jwang/github/part-ii-project/working/exp1/train/giza.co-or/co-or.A3.final
(3) generate word alignment @ Wed Oct 13 20:45:14 BST 2021
Combining forward and inverted alignment from files:
  /home/jwang/github/part-ii-project/working/exp1/train/giza.or-co/or-co.A3.final.{bz2,gz}
  /home/jwang/github/part-ii-project/working/exp1/train/giza.co-or/co-or.A3.final.{bz2,gz}
Executing: mkdir -p /home/jwang/github/part-ii-project/working/exp1/train/model
Executing: /home/jwang/github/mosesdecoder/scripts/training/giza2bal.pl -d "gzip -cd /home/jwang/github/part-ii-project/working/exp1/train/giza.co-or/co-or.A3.final.gz" -i "gzip -cd /home/jwang/github/part-ii-project/working/exp1/train/giza.or-co/or-co.A3.final.gz" |/home/jwang/github/mosesdecoder/scripts/../bin/symal -alignment="grow" -diagonal="yes" -final="yes" -both="yes" > /home/jwang/github/part-ii-project/working/exp1/train/model/aligned.grow-diag-final-and
symal: computing grow alignment: diagonal (1) final (1)both-uncovered (1)
skip=<0> counts=<28345>
(4) generate lexical translation table 0-0 @ Wed Oct 13 20:45:17 BST 2021
(/home/jwang/corpus/training/fce.train.gold.bea19.clean.or,/home/jwang/corpus/training/fce.train.gold.bea19.clean.co,/home/jwang/github/part-ii-project/working/exp1/train/model/lex)
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Saved: /home/jwang/github/part-ii-project/working/exp1/train/model/lex.f2e and /home/jwang/github/part-ii-project/working/exp1/train/model/lex.e2f
FILE: /home/jwang/corpus/training/fce.train.gold.bea19.clean.co
FILE: /home/jwang/corpus/training/fce.train.gold.bea19.clean.or
FILE: /home/jwang/github/part-ii-project/working/exp1/train/model/aligned.grow-diag-final-and
(5) extract phrases @ Wed Oct 13 20:45:21 BST 2021
/home/jwang/github/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /home/jwang/github/mosesdecoder/scripts/../bin/extract /home/jwang/corpus/training/fce.train.gold.bea19.clean.co /home/jwang/corpus/training/fce.train.gold.bea19.clean.or /home/jwang/github/part-ii-project/working/exp1/train/model/aligned.grow-diag-final-and /home/jwang/github/part-ii-project/working/exp1/train/model/extract 7 orientation --model phrase-msd --GZOutput 
Executing: /home/jwang/github/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /home/jwang/github/mosesdecoder/scripts/../bin/extract /home/jwang/corpus/training/fce.train.gold.bea19.clean.co /home/jwang/corpus/training/fce.train.gold.bea19.clean.or /home/jwang/github/part-ii-project/working/exp1/train/model/aligned.grow-diag-final-and /home/jwang/github/part-ii-project/working/exp1/train/model/extract 7 orientation --model phrase-msd --GZOutput 
MAX 7 1 0
Started Wed Oct 13 20:45:21 2021
using gzip 
isBSDSplit=0 
Executing: mkdir -p /home/jwang/github/part-ii-project/working/exp1/train/model/tmp.441; ls -l /home/jwang/github/part-ii-project/working/exp1/train/model/tmp.441 
total=28345 line-per-split=7087 
split -d -l 7087 -a 7 /home/jwang/corpus/training/fce.train.gold.bea19.clean.co /home/jwang/github/part-ii-project/working/exp1/train/model/tmp.441/target.split -d -l 7087 -a 7 /home/jwang/corpus/training/fce.train.gold.bea19.clean.or /home/jwang/github/part-ii-project/working/exp1/train/model/tmp.441/source.split -d -l 7087 -a 7 /home/jwang/github/part-ii-project/working/exp1/train/model/aligned.grow-diag-final-and /home/jwang/github/part-ii-project/working/exp1/train/model/tmp.441/align.merging extract / extract.inv
gunzip -c /home/jwang/github/part-ii-project/working/exp1/train/model/tmp.441/extract.0000000.gz /home/jwang/github/part-ii-project/working/exp1/train/model/tmp.441/extract.0000001.gz /home/jwang/github/part-ii-project/working/exp1/train/model/tmp.441/extract.0000002.gz /home/jwang/github/part-ii-project/working/exp1/train/model/tmp.441/extract.0000003.gz  | LC_ALL=C sort     -T /home/jwang/github/part-ii-project/working/exp1/train/model/tmp.441 2>> /dev/stderr | gzip -c > /home/jwang/github/part-ii-project/working/exp1/train/model/extract.sorted.gz 2>> /dev/stderr 
gunzip -c /home/jwang/github/part-ii-project/working/exp1/train/model/tmp.441/extract.0000000.inv.gz /home/jwang/github/part-ii-project/working/exp1/train/model/tmp.441/extract.0000001.inv.gz /home/jwang/github/part-ii-project/working/exp1/train/model/tmp.441/extract.0000002.inv.gz /home/jwang/github/part-ii-project/working/exp1/train/model/tmp.441/extract.0000003.inv.gz  | LC_ALL=C sort     -T /home/jwang/github/part-ii-project/working/exp1/train/model/tmp.441 2>> /dev/stderr | gzip -c > /home/jwang/github/part-ii-project/working/exp1/train/model/extract.inv.sorted.gz 2>> /dev/stderr 
gunzip -c /home/jwang/github/part-ii-project/working/exp1/train/model/tmp.441/extract.0000000.o.gz /home/jwang/github/part-ii-project/working/exp1/train/model/tmp.441/extract.0000001.o.gz /home/jwang/github/part-ii-project/working/exp1/train/model/tmp.441/extract.0000002.o.gz /home/jwang/github/part-ii-project/working/exp1/train/model/tmp.441/extract.0000003.o.gz  | LC_ALL=C sort     -T /home/jwang/github/part-ii-project/working/exp1/train/model/tmp.441 2>> /dev/stderr | gzip -c > /home/jwang/github/part-ii-project/working/exp1/train/model/extract.o.sorted.gz 2>> /dev/stderr 
Finished Wed Oct 13 20:45:45 2021
(6) score phrases @ Wed Oct 13 20:45:45 BST 2021
(6.1)  creating table half /home/jwang/github/part-ii-project/working/exp1/train/model/phrase-table.half.f2e @ Wed Oct 13 20:45:45 BST 2021
/home/jwang/github/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/jwang/github/mosesdecoder/scripts/../bin/score /home/jwang/github/part-ii-project/working/exp1/train/model/extract.sorted.gz /home/jwang/github/part-ii-project/working/exp1/train/model/lex.f2e /home/jwang/github/part-ii-project/working/exp1/train/model/phrase-table.half.f2e.gz  0 
Executing: /home/jwang/github/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/jwang/github/mosesdecoder/scripts/../bin/score /home/jwang/github/part-ii-project/working/exp1/train/model/extract.sorted.gz /home/jwang/github/part-ii-project/working/exp1/train/model/lex.f2e /home/jwang/github/part-ii-project/working/exp1/train/model/phrase-table.half.f2e.gz  0 
using gzip 
Started Wed Oct 13 20:45:46 2021
/home/jwang/github/mosesdecoder/scripts/../bin/score /home/jwang/github/part-ii-project/working/exp1/train/model/tmp.490/extract.0.gz /home/jwang/github/part-ii-project/working/exp1/train/model/lex.f2e /home/jwang/github/part-ii-project/working/exp1/train/model/tmp.490/phrase-table.half.0000000.gz  2>> /dev/stderr 
/home/jwang/github/part-ii-project/working/exp1/train/model/tmp.490/run.0.sh/home/jwang/github/part-ii-project/working/exp1/train/model/tmp.490/run.1.sh/home/jwang/github/part-ii-project/working/exp1/train/model/tmp.490/run.2.sh/home/jwang/github/part-ii-project/working/exp1/train/model/tmp.490/run.3.shmv /home/jwang/github/part-ii-project/working/exp1/train/model/tmp.490/phrase-table.half.0000000.gz /home/jwang/github/part-ii-project/working/exp1/train/model/phrase-table.half.f2e.gzrm -rf /home/jwang/github/part-ii-project/working/exp1/train/model/tmp.490 
Finished Wed Oct 13 20:46:24 2021
(6.3)  creating table half /home/jwang/github/part-ii-project/working/exp1/train/model/phrase-table.half.e2f @ Wed Oct 13 20:46:24 BST 2021
/home/jwang/github/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/jwang/github/mosesdecoder/scripts/../bin/score /home/jwang/github/part-ii-project/working/exp1/train/model/extract.inv.sorted.gz /home/jwang/github/part-ii-project/working/exp1/train/model/lex.e2f /home/jwang/github/part-ii-project/working/exp1/train/model/phrase-table.half.e2f.gz --Inverse 1 
Executing: /home/jwang/github/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/jwang/github/mosesdecoder/scripts/../bin/score /home/jwang/github/part-ii-project/working/exp1/train/model/extract.inv.sorted.gz /home/jwang/github/part-ii-project/working/exp1/train/model/lex.e2f /home/jwang/github/part-ii-project/working/exp1/train/model/phrase-table.half.e2f.gz --Inverse 1 
using gzip 
Started Wed Oct 13 20:46:24 2021
/home/jwang/github/mosesdecoder/scripts/../bin/score /home/jwang/github/part-ii-project/working/exp1/train/model/tmp.513/extract.0.gz /home/jwang/github/part-ii-project/working/exp1/train/model/lex.e2f /home/jwang/github/part-ii-project/working/exp1/train/model/tmp.513/phrase-table.half.0000000.gz --Inverse  2>> /dev/stderr 
/home/jwang/github/part-ii-project/working/exp1/train/model/tmp.513/run.0.sh/home/jwang/github/part-ii-project/working/exp1/train/model/tmp.513/run.1.sh/home/jwang/github/part-ii-project/working/exp1/train/model/tmp.513/run.2.sh/home/jwang/github/part-ii-project/working/exp1/train/model/tmp.513/run.3.shgunzip -c /home/jwang/github/part-ii-project/working/exp1/train/model/tmp.513/phrase-table.half.*.gz 2>> /dev/stderr| LC_ALL=C sort     -T /home/jwang/github/part-ii-project/working/exp1/train/model/tmp.513  | gzip -c > /home/jwang/github/part-ii-project/working/exp1/train/model/phrase-table.half.e2f.gz  2>> /dev/stderr rm -rf /home/jwang/github/part-ii-project/working/exp1/train/model/tmp.513 
Finished Wed Oct 13 20:47:08 2021
(6.6) consolidating the two halves @ Wed Oct 13 20:47:08 BST 2021
Executing: /home/jwang/github/mosesdecoder/scripts/../bin/consolidate /home/jwang/github/part-ii-project/working/exp1/train/model/phrase-table.half.f2e.gz /home/jwang/github/part-ii-project/working/exp1/train/model/phrase-table.half.e2f.gz /dev/stdout | gzip -c > /home/jwang/github/part-ii-project/working/exp1/train/model/phrase-table.gz
Consolidate v2.0 written by Philipp Koehn
consolidating direct and indirect rule tables
...............
Executing: rm -f /home/jwang/github/part-ii-project/working/exp1/train/model/phrase-table.half.*
(7) learn reordering model @ Wed Oct 13 20:47:23 BST 2021
(7.1) [no factors] learn reordering model @ Wed Oct 13 20:47:24 BST 2021
(7.2) building tables @ Wed Oct 13 20:47:24 BST 2021
Executing: /home/jwang/github/mosesdecoder/scripts/../bin/lexical-reordering-score /home/jwang/github/part-ii-project/working/exp1/train/model/extract.o.sorted.gz 0.5 /home/jwang/github/part-ii-project/working/exp1/train/model/reordering-table. --model "phrase msd phrase-msd-bidirectional-fe"
Lexical Reordering Scorer
scores lexical reordering models of several types (hierarchical, phrase-based and word-based-extraction
(8) learn generation model @ Wed Oct 13 20:47:36 BST 2021
  no generation model requested, skipping step
(9) create moses.ini @ Wed Oct 13 20:47:36 BST 2021
