% Template for a Computer Science Tripos Part II project dissertation
\documentclass[12pt,a4paper,twoside,openright]{report}
\usepackage[pdfborder={0 0 0}]{hyperref}    % turns references into hyperlinks
\usepackage[margin=25mm]{geometry}  % adjusts page layout
\usepackage{graphicx}  % allows inclusion of PDF, PNG and JPG images
\usepackage{pmboxdraw}
\usepackage{verbatim}
\usepackage{docmute}   % only needed to allow inclusion of proposal.tex
\usepackage{parskip}
\usepackage{todonotes}
\usepackage{listings}
\usepackage{amsmath}

\lstset{basicstyle=\ttfamily, breaklines=true}

\graphicspath{ {./images/} }

\newcommand{\quickwordcount}[1]{%
  \immediate\write18{texcount -1 -sum -merge -q #1.tex output.bbl > #1-words.sum }%
  \input{#1-words.sum}%
}

\raggedbottom                           % try to avoid widows and orphans
\sloppy
\clubpenalty1000%
\widowpenalty1000%

\renewcommand{\baselinestretch}{1.1}    % adjust line spacing to make
                                        % more readable

\begin{document}

\bibliographystyle{plain}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title


\pagestyle{empty}

\rightline{\LARGE \textbf{Jiaxin Wang}}

\vspace*{60mm}
\begin{center}
\Huge
\textbf{A Recursive Recurrent Neural Network Decoder for Grammatical Error Correction} \\[5mm]
Computer Science Tripos -- Part II \\[5mm]
Emmanuel College \\[5mm]
\today  % today's date
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proforma, table of contents and list of figures

\pagestyle{plain}

\chapter*{Proforma}

{\large
\begin{tabular}{ll}
Candidate Number:   & -                       \\
Project Title:      & A Recursive Recurrent Neural Network \\
 & Decoder for Grammatical Error Correction \\
Examination:        & Computer Science Tripos -- Part II, 2022  \\
Word Count:         & \quickwordcount{diss}\footnotemark[1] \\
Code line count:    & 1127 \\
Project Originator: & - \\
Supervisor:         & Dr Zheng Yuan, Dr Christopher Bryant
\end{tabular}
}
\footnotetext[1]{This word count was computed
by \texttt{detex diss.tex | tr -cd '0-9A-Za-z $\tt\backslash$n' | wc -w}
}
\stepcounter{footnote}


\section*{Original Aims of the Project}

-


\section*{Work Completed}

-

\section*{Special Difficulties}

-
 
\newpage
\section*{Declaration}

I, Jiaxin Wang of Emmanuel College, being a candidate for Part II of the Computer
Science Tripos, hereby declare that this dissertation and the work described in 
it are my own work, unaided except as may be specified below, and that the dissertation
does not contain material that has already been used to any substantial
extent for a comparable purpose.

\bigskip
\leftline{Signed [signature]}

\medskip
\leftline{Date [date]}

\tableofcontents

\listoffigures

\newpage
\section*{Acknowledgements}

-

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% now for the chapters
\listoftodos

\pagestyle{headings}

\chapter{Introduction}

This project concerns my implementation of a \textit{recursive recurrent neural network} model (R\textsuperscript{2}NN) proposed by Liu et al. (2014) \cite{r2nn} This is to be integrated in a statistical machine translation (SMT) system to attempt the task of grammatical error correction. \todo[inline]{More details}

\section{Motivation}

Grammatical Error Correction (GEC) is the task of producing a grammatically correct sentence given a potentially erroneous text while preserving its meaning. The main motivation behind this task lies in its role in helping learners of a foreign language understand the language better. In addition, native speakers can make use of such GEC tools to avoid mistakes in their writing. Being an ESL (English as a second language) learner myself, I would like to investigate models that can be used for the task of GEC in English, and I hope that it can benefit people who may find GEC tools useful.

\todo[inline]{Technical motivations: why R2NN? Academic motivation: publishing code for R2NN as no code available}


\section{Problem Overview}

The task of Grammatical Error Correction can be seen as a machine translation process, where the input is a text which may contain grammatical errors, and the output is an error-free text. This project uses a statistical machine translation (SMT) approach to solve GEC.

A recursive recurrent neural network (R\textsuperscript{2}NN) was proposed by Liu et al. (2014) \cite{r2nn} for SMT. The goal of this project is to implement the proposed model to be used for GEC. The model should aim to correct all types of errors, namely grammatical, lexical, and orthographical errors. Its performance will be evaluated against a baseline SMT system, Moses\cite{moses}.


\section{Related Work}
-

\chapter{Preparation}

\section{Starting Point}

This project is based on the idea presented in the paper \textit{A Recursive Recurrent Neural Network for Statistical Machine Translation} \cite{r2nn}. An R\textsuperscript{2}NN model was proposed, but the implementation details are not given in the paper.

The Part IB Computer Science Tripos course Artificial Intelligence\footnote{https://www.cl.cam.ac.uk/teaching/2021/ArtInt/} gives an introduction to neural networks and explains how forwarding and backpropagation works. Prior to this project I did not have any coding experience with neural networks. I have found it useful to follow the coding examples on PyTorch\footnote{https://pytorch.org/tutorials/} website.

An example SMT system is available on Moses\footnote{https://www.statmt.org/moses/?n=Moses.Baseline} website. It provides a detailed tutorial on how to install Moses, how to prepare corpus and how to train the SMT.


\section{Theory}

\subsection{SMT system}
A typical SMT system consists of four main components: The language model (LM), the translation model (TM), the reordering model and the decoder\cite{yuan2017phd}. The LM computes the probability of a given sequence being valid. The TM builds a translation table which contains mappings of words/phrases between source and target corpora. The reordering model learns about phrase reordering of translation. The decoder finds a translation candidate who is most likely to be the translation of the source sentence. In the task of GEC, this would be the most probable correction to the original erroneous sentence.

\todo[inline]{Level of detail?}


\subsection{Moses SMT}
The Moses baseline system uses KenLM as the language model. For the translation model, it uses GIZA++ to obtain a word alignment model. The model should be trained to produce a phrase table and associated scores. Reordering tables are also created during this process. Eventually, the Moses decoder will find the best translation candidate given input based on the scores it calculated in previous stages.

\subsection{Feedforward Neural Network}
A feedforward neural network consists of three parts: an input layer, one or more hidden layers, and an output layer. In a feedforward neural network, data only flows in one direction (forward) from input to output. Figure \ref{fig:fnn} shows an example of a feedforward neural network with one hidden layer.

\begin{figure}[ht]
\centering
\includegraphics[width=0.3\textwidth]{images/fnn.png}
\caption{Example of a feedforward neural network}
\label{fig:fnn}
\end{figure}

\subsection{Recurrent Neural Network (RNN)}
Recurrent neural networks are usually used to deal with sequences. They allow access to the input data as well as past data to compute the next state. As shown in Figure \ref{fig:rnn}, the hidden layer $h_t$ is computed using both input $x_t$ at time $t$ and the hidden state $h_{t-1}$ which contains the history information from time 0 to $t-1$. For each timestep $t$, the hidden state can be expressed as:
\[ h_t = Wx_t + Uh_{t-1} \]

\begin{figure}[ht]
\centering
\includegraphics[width=0.4\textwidth]{images/rnn.png}
\caption{Example of a recurrent neural network (RNN)}
\label{fig:rnn}
\end{figure}


\subsection{Recursive Neural Network (RvNN)}
A recursive neural network has a tree-like structure. Figure \ref{fig:rvnn} illustrates an example of a basic RvNN architecture. The parent node representation is computed from its child nodes' representation as follows:
\[ p_{n, n+1} = f(W[x_n; x_{n+1}]) \]
where $f$ is the activation function. The same weight matrix $W$ will be applied recursively over the input.

\begin{figure}[ht]
\centering
\includegraphics[width=0.6\textwidth]{images/rvnn.png}
\caption{Example of a recursive neural network (RvNN)}
\label{fig:rvnn}
\end{figure}


\subsection{\texorpdfstring{Recursive Recurrent Neural Network (R\textsuperscript{2}NN)}{Recursive Recurrent Neural Network (R2NN)}}
The R\textsuperscript{2}NN proposed by Liu et al.\cite{r2nn} combines the features of RvNN and RNN. It has a tree-like structure similar to RvNN, with recurrent vectors added to integrate global information. As shown in Figure \ref{fig:r2nn}, $s^{[l, m]}$ and $s^{[m, n]}$ is the representation of child nodes $[l, m]$ and $[m, n]$. The recurrent input vectors, $x^{[l, m]}$ and $x^{[m, n]}$ are added to the two child nodes respectively. They encode the global information, such as language model scores and distortion model scores. A third recurrent input vector $x^{[l, n]}$ is added to the parent node $[l, n]$. The parent node representation is computed as
\[ s_j^{[l, n]} = f(\sum_{i} \hat{x}_i^{[l, n]}w_{ji}) \]
where $\hat{x}$ is the concatenation of vectors $[x^{[l, m]}; s^{[l, m]}; x^{[m, n]}; s^{[m, n]}]$, and $f$ is the $HTanh$ function. The output, $y^{[l, n]}$, is computed as
\[ y^{[l, n]} = \sum_{j} ([s^{[l, n]}; x^{[l, n]}])_{j}v_j \]


\begin{figure}[ht]
\centering
\includegraphics[width=0.6\textwidth]{images/r2nn.png}
\caption{Recursive recurrent neural network (Liu et al., 2014, p.1494)}
\label{fig:r2nn}
\end{figure}


\section{Requirement Analysis}

Based on the Project Structure section from my project proposal, the following requirements have been identified:

\textbf{Data preprocessing}
\begin{itemize}
    \item Data should be prepared in a form that is accepted by Moses SMT and R\textsuperscript{2}NN SMT
    \item Preprocessing should be done carefully to avoid accidentally correcting some of the grammatical errors 
    
    \begin{itemize}
        \item E.g. capitalisation errors may go undetected if all sentences are lowercased during data preprocessing
    \end{itemize}
\end{itemize}

\textbf{Moses SMT for GEC}
\begin{itemize}
    \item A language model should be trained to model the probability of a given sentence being valid
    \item A translation model should be trained to construct a phrase translation table
    \item A reordering model should be trained to learn the reordering of phrases
    \item With the above three models and Moses decoder, a complete Moses SMT system should be built
\end{itemize}

\textbf{R\textsuperscript{2}NN SMT for GEC}

Following the R\textsuperscript{2}NN paper by Liu et al.\cite{r2nn},
\begin{itemize}
    \item Phrase pair embeddings (PPE) should be learned by building a one-hidden-layer neural network and a recurrent neural network
    \item A recursive recurrent neural network (R\textsuperscript{2}NN) should be built and used as a decoder
\end{itemize}

\textbf{Evaluation}
\begin{itemize}
    \item The performance of both SMT systems should be evaluated using F0.5 scores
\end{itemize}

\section{Choice of Tools}

\subsection{Programming Language}

Python is chosen to be the main programming language as it provides many libraries that are commonly used for natural language processing. For this project I will be using \texttt{python 3.8} and PyCharm as my IDE.

\subsection{Libraries}
\textbf{PyTorch} \\
The \texttt{PyTorch}\footnote{https://pytorch.org/} library is one of the most popular machine learning frameworks. There are other similar libraries (such as \texttt{TensorFlow}) but I find \texttt{PyTorch} tutorials are easier to follow.

\textbf{NumPy} \\
My project is likely to involve statistical processing. I would be using the \texttt{NumPy}\footnote{https://numpy.org/} library for this purpose.

\textbf{pandas} \\
\texttt{pandas}\footnote{https://pandas.pydata.org/} is a powerful library for processing tabular data. This would be used to manipulate phrase tables in my project.

\subsection{Dataset}
The dataset introduced in BEA 2019 Shared Task\cite{bryant-etal-2019-bea} will be used in this project. I chose the corpora (\textbf{FCE v2.1}) which is immediately downloadable from the website\footnote{https://www.cl.cam.ac.uk/research/nl/bea2019st/\#data} to start with. The corpora have been standardised to be easily evaluated by ERRANT\cite{bryant-etal-2017-automatic,felice-etal-2016-automatic}. ERRANT is a toolkit used to annotate parallel data and compare hypothesis against reference to produce various evaluation metrics including F0.5 score. I may request other corpora as an extension of my project.

For language model training, I will be using the \textbf{One Billion Word} dataset\cite{one-billion-word}. This is a dataset used for language modeling and is available on GitHub\footnote{https://github.com/ciprian-chelba/1-billion-word-language-modeling-benchmark}.

\subsection{Version Control and Backup}
Git will be used for version control. The entire project, including all the written code and my dissertation, will be pushed to GitHub regularly.

\chapter{Implementation} \label{implementation}
This chapter describes the implementation of a Moses baseline SMT system and a R\textsuperscript{2}NN SMT system. Since the main purpose of this project is to compare the performance of our R\textsuperscript{2}NN decoder against Moses decoder, the R\textsuperscript{2}NN system should use the same language model, translation model and reordering model as Moses. For the translation model, the R\textsuperscript{2}NN paper\cite{r2nn} proposed a \textit{translation confidence based phrase pair embedding} (TCBPPE) to be used with the R\textsuperscript{2}NN decoder. The TCBPPE will be based on a phrase table which is produced by Moses translation model. In the end, the R\textsuperscript{2}NN decoder will make use of language model scores, translation model scores, reordering model scores from Moses as well as TCBPPE to find the best translation candidate.

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\textwidth]{images/overview_moses_r2nn.png}
\caption{Overview of Moses SMT and R\textsuperscript{2}NN SMT}
\label{fig:moses_r2nn}
\end{figure}

\section{Setup}

\subsubsection{FCE Dataset}

The FCE dataset was provided in m2 format. However, Moses requires parallel data which is aligned at the sentence level. An example taken from the m2 file looks like this:

\texttt{S Her friend Pat had explained the whole story at her husband .\\
A 8 9|||R:PREP|||to|||REQUIRED|||-NONE-|||0}

[This means that for source sentence S, the word at position 8 to 9 (\texttt{at}) should be corrected to \texttt{to}.]

The m2 format needs to be converted to sentence-aligned data to be used by Moses. Two files are generated from this, where the source file contains\\
\texttt{Her friend Pat had explained the whole story at her husband .}

and the target file contains\\
\texttt{Her friend Pat had explained the whole story to her husband .}

\subsubsection{One Billion Word Dataset}

I downloaded the One Billion Word dataset for language model training. However, the files were too large to be handled by my machine. Since the dataset consists of fifty files, I decided to randomly choose 10 files from them and concatenated these files to be used in language model training. 


\section{Moses Baseline}

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\textwidth]{images/moses_pipeline.png}
\caption{Overview of Moses SMT}
\label{fig:moses_pipeline}
\end{figure}

\subsection{Language Model}
The One Billion Word dataset is used for language model training. In the R\textsuperscript{2}NN paper, a 5-gram language model is used. Here, I used KenLM which came with Moses installation to train a 5-gram language model. An example query of the trained language model looks like this:
\todo[inline]{Example query of LM}

\subsection{Translation Model and Reordering Model}
The translation model is trained with FCE sentence-aligned data. There are three components in translation model training: word alignment, phrase extraction and scoring. Word alignment is obtained using GIZA++, a toolkit to train word alignment models. Phrases are then extracted from our training files and a phrase table is produced. The phrase table contains phrase translation pairs and the scores associated with each pair.

\begin{table}[h!]
\centering
\begin{tabular}{ |c|c|c|c|c| } 
 \hline
 source & target & scores & alignment & counts \\ [0.5ex] 
 \hline
 As we & As we & 1 0.901657 0.846154 0.90511 & 0-0 1-1 & 11 13 11\\
 As we & We & 0.00115075 1.68879e-05 0.0769231 0.0153198 & 1-0 & 869 13 1\\
 As we & as we & 0.0212766 0.00117032 0.0769231 0.00482726 & 0-0 1-1 & 47 13 1\\
 \hline
\end{tabular}
\caption{Phrase table from Moses}
\label{table:phrase_table}
\end{table}

A distance-based reordering model is then built, and a reordering table is created. Finally, Moses decoder would make use of the language model, the phrase table as well as the reordering table. Given a source sentence, the Moses decoder will output the best translation candidate based on the scores from the above models. 
\todo[inline]{More details}

\section{\texorpdfstring{R\textsuperscript{2}NN}{R2NN} SMT}

\begin{figure}[ht]
\centering
\includegraphics[width=1\textwidth]{images/r2nn_pipeline.png}
\caption{Overview of R\textsuperscript{2}NN SMT}
\label{fig:r2nn_pipeline}
\end{figure}

The R\textsuperscript{2}NN SMT consists of three parts: translation confidence based phrase pair embedding (TCBPPE), global features, and a R\textsuperscript{2}NN decoder.

\subsubsection{TCBPPE}
A phrase pair embedding is a vector representation which encodes the meaning of a phrase pair. Recall that our R\textsuperscript{2}NN model has a tree structure, where each node has a representation vector $s$ and a recurrent vector $x$. The TCBPPE is to be used as the representation vector $s$ and it is used to generate the leaf nodes of the derivation tree. 

The phrase pair embedding is split into two parts: translation confidence with sparse features and translation confidence with recurrent neural network. These two vectors will be obtained separately, and will be concatenated to be used as the representation vector $s$ in the R\textsuperscript{2}NN.

\subsubsection{Global features}
The global features encode global information that cannot be generated by child representations. It includes language model scores, translation scores and reordering scores which are obtained from Moses. These scores are concatenated together to be used as recurrent vectors $x$ to the R\textsuperscript{2}NN model.

\subsubsection{R\textsuperscript{2}NN decoder}
The recursive recurrent neural network (R\textsuperscript{2}NN) will be used as a decoder to find the best translation candidate. For an input sentence, the decoder would construct a tree based on phrases in the sentence. An output score will be computed based on TCBPPE, global features and the structure of the tree. The translation candidate that gives the highest score will be taken as the best translation candidate.

\subsection{TCBPPE: Sparse Features} \label{TCBPPE_sparse}

\begin{figure}[ht]
\centering
\includegraphics[width=0.3\textwidth]{images/one_hidden_layer.png}
\caption{Structure of one-hidden-layer feedforward neural network}
\label{fig:one_hidden_layer}
\end{figure}

A one-hidden-layer feedforward neural network is used to get a translation confidence score using sparse features. The structure of the network is shown in Figure \ref{fig:one_hidden_layer}. Following the R\textsuperscript{2}NN paper\cite{r2nn}, the size of the hidden layer is set to 20. The size of the input layer is 200,001. By training the network, we obtain hidden matrix $W$ of size 200,001 by 20. $W$ will be used as phrase pair embedding matrix for sparse features (\texttt{ppe\_sparse}). The goal is to find the $W$ that gives the best phrase pair embedding.

\subsubsection{Top phrase table}
A top phrase table \texttt{pt\_top} will be used when encoding training data. It contains the top 200,000 most frequent phrase pairs that appeared in the training data. We can obtain \texttt{pt\_top} from Moses phrase table using \texttt{pandas}. Recall that a column named "counts" is given in the Moses phrase table (Table \ref{table:phrase_table}). It contains three numbers: the first is the number of times the target phrase appeared in training target file; the second is the number of times the source phrase appeared in training source file; the third is the number of times that the source phrase is mapped to the target phrase. To find the 200,000 most frequent phrase pairs, we can use \texttt{pandas} to perform a sort on "counts", and keep the top 200,000 rows:

\begin{verbatim}
import pandas as pd
pt = pd.read_csv(path_to_pt_rw, sep='|', 
            names=["source", "target", "scores", "alignment", "count"], 
            quoting=3)
pt[["count_co", "count_or", "count_or_co"]] 
                = pt["count"].str.split(expand=True)
...
pt_sort = pt.sort_values(["count_or_co", "count_co", "count_or"], 
                        ascending=False)
most_freq_pt = pt_sort.head(200000)
\end{verbatim}

This would sort the counts column in descending order, with priority of
\[\textrm{phrase pair count} > \textrm{target count} > \textrm{source count}\]

\subsubsection{Prepare training data}
Next, we need to encode the training data (sentences) as one-hot like data to be used as input to the network. The phrase pairs in \texttt{pt\_top} will each be a feature, and an additional feature is used for all the infrequent phrase pairs. For each sentence pair (source-target) in the training data, it needs to be encoded as a vector of length 200,001 consisting of 0s and 1s, where the position $i$ in the vector contains a 1 if and only if the $i$\textsuperscript{th} most frequent phrase pair is found in this sentence pair. An example taken from the training file:

[Source sentence]
\texttt{What she would do ?}

[Target sentence]
\texttt{What would she do ?}

[Encoded vector]
\texttt{[$\underset{0}{0}, \ldots, \underset{39}{1}, \ldots, \underset{200000}{1}$]}

A 0 at position 0 means that the most frequent phrase pair does not exist in the sentence pair. A 1 at position 39 means that the 40\textsuperscript{th} most frequent phrase pair exists in the sentence pair. A 1 at position 200,000 means that the sentence pair contains a phrase pair that is not in the top 200,000 most frequent pairs.

\begin{table}[h!]
\centering
\begin{tabular}{ |c|c|c|c| } 
 \hline
 index & source & target & \ldots \\ [0.5ex] 
 \hline
 0 & . & . & \ldots \\ 
 \ldots & \ldots & \ldots & \ldots \\ 
 39 & do & do & \ldots \\ 
 \hline
\end{tabular}
\caption{Top 200,000 phrase pairs}
\label{table:pt_top_exmaple}
\end{table}

One problem with this is that the resulting vector from sentence pairs are usually sparse vectors. To save memory, instead of storing the whole vector, the position indices of 1s in the vector are stored in a file named \texttt{phrase\_pair\_id} (i.e. the phrase pair ids used in each sentence pair are stored). When loading the sentence pairs as training data, it is easy to obtain the one-hot encoding of each sentence pair from \texttt{phrase\_pair\_id}.

The next step is to obtain the expected output for each input sentence pair. Since the TCBPPE encodes translation model information, it is reasonable to use the translation scores from Moses translation model. For each sentence pair $S\textrm{-}T$ containing phrase pairs $p_1, p_2, ..., p_n$, the expected output score is computed by
\[ score_{S\textrm{-}T} = \frac{\sum_n c_{p_n}}{n} \]
where $c_{p_n}$ is the average translation score for phrase pair $p_n$.

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\textwidth]{images/sparse_pipeline.png}
\caption{Training pipeline of one-hidden-layer feedforward neural network}
\label{fig:sparse_pipeline}
\end{figure}

\subsubsection{Training one-hidden-layer neural network}

After obtaining the training dataset, we can train the neural network. The neural network will be trained for 10 epochs, where one epoch refers to one cycle of processing all the data in training set. After each epoch, the hidden matrix $W$ is stored. The $k^{th}$ row in $W$ will be used as the phrase pair embedding (sparse) for the top $k^{th}$ frequent phrase pair. That is, the embedding for the most frequent phrase pair is $W[0]$, and the embedding for phrase pairs that are not in the top 200,000 is represented by $W[200000]$.

We evaluate $W$ by updating the Moses phrase table: for each row (ppe) in $W$, we add the ppe as an additional feature to the phrase table. Then we pass the updated phrase table to Moses decoder and use ERRANT to evaluate the performance. Table \ref{table:sparse_5} and Table \ref{table:sparse_10} shows the evaluation result by ERRANT for training epoch 5 and training epoch 10 respectively. Among all the $W$ from epoch 1 to 10, 10-epoch $W$ gives the highest precision, recall and $F_{0.5}$ score. Hence, $W$ will be used as the ppe matrix for sparse features (\texttt{ppe\_sparse}).

\begin{table}[h!]
\centering
\begin{tabular}{ |c|c|c|c|c|c| } 
 \hline
 TP & FP & FN & Prec & Rec & F0.5 \\ [0.5ex] 
 \hline
 543 & 2366 & 4006 & 0.1867 & 0.1194 & 0.1677 \\ 
 \hline
\end{tabular}
\caption{ERRANT output (5 epochs)}
\label{table:sparse_5}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{ |c|c|c|c|c|c| } 
 \hline
 TP & FP & FN & Prec & Rec & F0.5 \\ [0.5ex] 
 \hline
 545 & 2363 & 4004 & 0.1874 & 0.1198 & 0.1684 \\ 
 \hline
\end{tabular}
\caption{ERRANT output (10 epochs)}
\label{table:sparse_10}
\end{table}


\subsection{TCBPPE: Recurrent Neural Network (RNN)} \label{TCBPPE_rnn}

\begin{figure}[ht]
\centering
\includegraphics[width=0.4\textwidth]{images/rnn_ppe.png}
\caption{Recurrent neural network for translation confidence (Liu et al., 2014, p.1497)}
\label{fig:rnn_ppe}
\end{figure}

The other part of TCBPPE is generated by a recurrent neural network (RNN). The structure of the RNN used in shown in Figure \ref{fig:rnn_ppe}, where $e_i$ is the source word embedding, and $f_{a_i}$ is the word embedding of the target word which is aligned to $e_i$. The translation confidence score from source $s$ to target $t$ is given by
\[ T_{S2T}(s, t) = \sum_{i} \log p(e_i|e_{i-1}, f_{a_i}, h_i)\]

\subsubsection{Building RNN}
Firstly, we need to decide what to use for word embeddings. There are several popular pre-trained word embeddings available, such as GloVe\footnote{https://nlp.stanford.edu/projects/glove/} and fastText\footnote{https://fasttext.cc/docs/en/english-vectors.html}. I have chosen fastText pre-trained models because fastText is based on character n-grams, whereas GloVe takes a word to be the smallest unit. For the task of Grammatical Error Correction, it is likely that OOV (out of vocabulary) words will occur in the source (such as misspelled words). fastText can easily generate word embeddings for those unseen words based on character n-grams.

Next step is to construct a PyTorch dataset from the FCE training data. For each $S\textrm{-}T$ sentence pair, we obtain $e_i$ and $e_{i-1}$ from source $S$ and $f_{a_i}$ from target $T$. Then we define a class for RNN as follows:

\begin{verbatim}
    class RecurrentNN(torch.nn.Module):
        def __init__(self, input_size, hidden_size, output_size):
            super(RecurrentNN, self).__init__()
            self.U = torch.nn.Linear(input_size+hidden_size, hidden_size)
            self.V = torch.nn.Linear(hidden_size, output_size)
            self.softmax = torch.nn.LogSoftmax(dim=1)
\end{verbatim}

where \texttt{input\_size} should be size of word embedding multiplied by 2 (since it is $e_{i-1}$ and $f_{a_i}$ concatenated together). The size of word embedding is set to 50 to avoid memory issues, and the size of hidden layer is set to 100. The \texttt{torch.nn.LogSoftmax} layer would convert output to a categorical probability distribution, hence the \texttt{output\_size} should be the size of vocabulary in our training data.

\subsubsection{Training RNN}
We train the RNN for 3 epochs and store the model state. Then, we load the model and run the model on the top phrase table \texttt{pt\_top}. For each phrase pair from source phrase $s$ to target phrase $t$, we compute 
\[ T_{S2T}(s, t) = \sum_{i} \log p(e_i|e_{i-1}, f_{a_i}, h_i)\]
and save $T_{S2T}(s, t)$ to be the phrase pair embedding with RNN (\texttt{ppe\_rnn}). 

\subsection{Global Features} \label{global_features}
Three features are proposed to be used as global features, namely language model score, translation model score and reordering model score. Language model scores can be obtained by querying the trained 5-gram KenLM language model. Translation model scores are accessible from the Moses phrase table. However, although a reordering table was created by Moses after training, it is unclear what each score means and the Moses documentation did not give details of the reordering score components. Hence, the reordering scores are omitted and only translation model score and language model score will be used as global features in R\textsuperscript{2}NN decoder.

\subsubsection{Translation model scores}
The translation model scores are calculated by taking the average of the four scores given by each phrase pair in the phrase table. If \texttt{pt} is the phrase table loaded by \texttt{pandas}, we could calculate the translation model score for each phrase pair by

\begin{verbatim}
    pt["confidence"] = pt["scores"].map(
                        lambda scores: round(
                        sum([float(i) for i in scores.split()])/4, 6))
\end{verbatim}

The result will be stored in a file named \texttt{pt\_top\_id\_confidence}.

\subsubsection{Language model scores}
We obtain the language model scores for the target phrase by querying the language model:
\begin{verbatim}
    ~/github/mosesdecoder/bin/query -n 
    /path/to/billion-training-monolingual-10.blm.en 
    < pt_top_target
    > lm_total_scores
\end{verbatim}

where \texttt{billion-training-monolingual-10.blm.en} is the binarised trained language model, \texttt{pt\_top\_target} is the file containing target phrases, one per line, and the output of this command is saved in \texttt{lm\_total\_scores}. We can write the language model score back to the phrase table and name it \texttt{pt\_top\_lm}.


\subsection{\texorpdfstring{R\textsuperscript{2}NN}{R2NN} Decoder} \label{func_ppe_rec}
Recall that in Figure \ref{fig:r2nn}, each node in R\textsuperscript{2}NN consists of two parts: $s$ for the representation of the phrase (TCBPPE), and $x$ for the recurrent input vectors (global features). Let's first define two functions, \texttt{get\_ppe} and \texttt{get\_rec} which would return $s$ and $x$ respectively for any given source phrase and target phrase.

\subsubsection{Obtain representation vector (TCBPPE)}
We obtained two files, \texttt{ppe\_sparse} and \texttt{ppe\_rnn}, from section \ref{TCBPPE_sparse} and \ref{TCBPPE_rnn}. We combine them to be the TCBPPE (\texttt{ppe\_matrix}):

\begin{verbatim}
    ppe_matrix = np.zeros((200001, 21))
    ppe_matrix[:, 0:20] = ppe_sparse
    ppe_matrix[0:200000, 20] = ppe_rnn
\end{verbatim}

and the phrase pair embedding for the top $i^{th}$ phrase pair is accessed by \texttt{ppe\_matrix[i]} and is of length 21. \texttt{ppe\_matrix[200000]} is used for any phrase pair that is not in the top 200,000 frequent ones. We can define \texttt{get\_ppe} as

\begin{verbatim}
    def get_ppe(source_phrase, target_phrase):
        mappings = pt_top_lm.loc[(pt_top_lm["source"] == source_phrase) 
                                & (pt_top_lm["target"] == target_phrase)]
        ...
\end{verbatim}

and return \texttt{ppe\_matrix[200000]} if \texttt{mappings} is empty (i.e. no such phrase pair found in the top phrase table), otherwise return \texttt{ppe\_matrix[mapping\_id]}.

\subsubsection{Obtain recurrent vector (Global features)}
The recurrent vector consists of two scores: the translation model score and the language model score. From section \ref{global_features}, we created \texttt{pt\_top\_id\_confidence} for translation model scores, and \texttt{pt\_top\_lm} for language model scores. With the help of these files, we can define \texttt{get\_rec}. Similar to \texttt{get\_ppe}, we first obain mappings from source phrase to target phrase. Then we look up the scores in \texttt{pt\_top\_id\_confidence} and \texttt{pt\_top\_lm} using \texttt{mapping\_id}.

\begin{verbatim}
    def get_rec(source_phrase, target_phrase):
        mappings = pt_top_lm.loc[(pt_top_lm["source"] == source_phrase) 
                                & (pt_top_lm["target"] == target_phrase)]
        ...
        t_score = pt_id_confidence.iloc[mapping_id]["confidence"]
        l_score = pt_top_lm.iloc[mapping_id]["lm"]
        return torch.tensor([t_score, l_score], dtype=torch.float32)
\end{verbatim}

If \texttt{mappings} is empty, both \texttt{t\_score} and \texttt{l\_score} will default to 0.

\subsubsection{Building recursive neural network (RvNN)}
Since R\textsuperscript{2}NN has a tree-like structure, I started with building a recursive neural network. A good example is given in a blog post\cite{rvnn_daniel}, where the recursive neural network is constructed from two parts: a class \texttt{TreeNode} which builds a greedy tree from the input, and a class \texttt{NeuralGrammar} which is the neural network that processes the tree. Following the post, I built a recursive neural network (RvNN) with similar structure:

\begin{verbatim}
    class RecursiveNN(torch.nn.Module):
        def __init__(self, input_size):
            super(RecursiveNN, self).__init__()
            self.W = torch.nn.Linear(input_size * 2, input_size)
            self.V = torch.nn.Linear(input_size, 1)
    
        def forward(self, node):
            ...
    
    class TreeNode:
        def __init__(self, representation=None):
            self.representation = representation
            self.left = None
            self.right = None
    
        def greedy_tree(self, x, model):
            ...
\end{verbatim}

The \texttt{greedy\_tree} function in class \texttt{TreeNode} takes argument \texttt{x} which is a list of representation of leaf nodes. It converts each leaf node representation to a \texttt{TreeNode} first, and then combine them in a greedy way. Based on the output of \texttt{model(combined\_tree)}, it chooses the combination of the nodes that gives the highest score.

To use \texttt{RecursiveNN}, we construct a tree from representation of leaf nodes $x$ and pass the tree to the neural network:

\begin{verbatim}
    rvnn = RecursiveNN(4)
    x = numpy.array([[8,8,8,8], [2,2,2,2], [3,3,3,3], [7,7,7,7]])
    tree_node = TreeNode()
    tree = tree_node.greedy_tree(x, rvnn)
    parent_node, score = rvnn(tree)
\end{verbatim}


\subsubsection{Building \texorpdfstring{R\textsuperscript{2}NN}{R2NN}}
To build R\textsuperscript{2}NN, we need to add recurrent input vector to the RvNN. Firstly, \texttt{ppe\_size} and \texttt{rec\_size} should be included in the initialisation signature. The size of $W$ should be (\texttt{ppe\_size} $+$ \texttt{rec\_size}) by \texttt{ppe\_size}, since the phrase pair embedding for parent node is generated from child nodes, whereas the recurrent input vector for parent node is independent of child nodes. The size of $V$ is (\texttt{ppe\_size} $+$ \texttt{rec\_size}) by 1 because output score is of size 1 and is generated from concatenation of phrase pair embedding and recurrent vector of parent node.

\begin{verbatim}
    class R2NN(torch.nn.Module):
        def __init__(self, ppe_size, rec_size):
            super(R2NN, self).__init__()
            self.input_size = ppe_size + rec_size
            self.W = torch.nn.Linear(self.input_size*2, ppe_size)
            self.V = torch.nn.Linear(self.input_size, 1)
\end{verbatim}

The \texttt{forward} function should use the \texttt{get\_rec} function defined in section \ref{func_ppe_rec} to obtain the recurrent input vector for parent node.

\begin{verbatim}
    def forward(self, node):
        ...
        parent_ppe = torch.tanh(self.W(inp_vector))
        parent_rec = get_rec(parent_source, parent_target)
        ...
\end{verbatim}

At the same time, when \texttt{greedy\_tree} function generate leaf tree nodes, it will make use of both \texttt{get\_ppe} and \texttt{get\_rec} to get tree node vector representation.

\subsubsection{Preprocessing training data}
Following the R\textsuperscript{2}NN paper\cite{r2nn}, forced decoding will be used to generate positive samples. We can perform forced decoding with Moses by adding the following line to Moses configuration file, \texttt{moses.ini}:
\begin{verbatim}
    ConstrainedDecoding path=/path/to/gold/standard/file
\end{verbatim}
where the gold standard file should be target sentences in FCE training dataset. Then we can pass the updated configuration file to Moses decoder and do forced decoding on source sentences in FCE training set. The phrase pairs used in forced decoding for each sentence is stored in a file named \texttt{phrases\_train}.

We define a function \texttt{process\_phrases\_file} to remove the sentences which did not give a forced decoding result in \texttt{phrases\_train}. For each of the remaining sentences, we extract phrase pairs used and a total score given by Moses decoder. Then we can construct a PyTorch dataset where the input is a list of phrase pairs used in each sentence and the expected output is the total score of that sentence.

\subsubsection{Training \texorpdfstring{R\textsuperscript{2}NN}{R2NN}}
We recall the loss function defined in R\textsuperscript{2}NN paper\cite{r2nn}
\[Loss(W,V,s^{[l,n]}) = \max{(0, 1-y_{oracle}^{[l,n]} + y_t^{[l,n]})} \]
where $y_{oracle}^{[l,n]}$ is the score from forced decoding (the expected score), and $y_t^{[l,n]}$ is the output score from R\textsuperscript{2}NN for a source span $s^{[l,n]}$. The $\max$ function is used to guarantee non-negative loss. Then we save the state of our trained model in \texttt{model\_state\_r2nn.pth}.

\section{Testing}

\section{Repository Overview}

\chapter{Evaluation}

\chapter{Conclusion}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the bibliography
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{refs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the appendices
\appendix

\chapter{Project Proposal}

% \chapter{Latex source}

% \section{diss.tex}
% {\scriptsize\lstinputlisting[language=TeX]{diss.tex}}

% \section{proposal.tex}
% {\scriptsize\verbatiminput{proposal.tex}}

% \chapter{Makefile}

% \section{refs.bib}
% {\scriptsize\verbatiminput{refs.bib}}


% \chapter{Project Proposal}

% \input{proposal}

\end{document}