# MERT optimized configuration
# decoder /home/jwang/github/mosesdecoder/bin/moses
# BLEU 0.79014 on dev /home/jwang/github/part-ii-project/corpus/dev/fce.dev.gold.bea19.or
# We were before running iteration 9
# finished Fri Oct 15 17:37:36 BST 2021
### MOSES CONFIG FILE ###
#########################

# input factors
[input-factors]
0

# mapping steps
[mapping]
0 T 0

[distortion-limit]
6

# feature functions
[feature]
UnknownWordPenalty
WordPenalty
PhrasePenalty
PhraseDictionaryMemory name=TranslationModel0 num-features=5 path=/home/jwang/github/part-ii-project/model/sparse/phrase_tables/phrase_table_10.gz input-factor=0 output-factor=0
LexicalReordering name=LexicalReordering0 num-features=6 type=phrase-msd-bidirectional-fe-allff input-factor=0 output-factor=0 path=/home/jwang/github/part-ii-project/moses_exp/exp2/train/model/reordering-table.phrase-msd-bidirectional-fe.gz
Distortion
KENLM name=LM0 factor=0 path=/home/jwang/github/part-ii-project/lm/billion-training-monolingual-10.blm.en order=5
ConstrainedDecoding path=/home/jwang/github/part-ii-project/corpus/training/fce.train.gold.bea19.co max-unknowns=3

# dense weights for feature functions

[threads]
4
[weight]

LexicalReordering0= 0.249375 0.0415777 0.0305098 0.00539419 0.0545741 0.0132137
Distortion0= 0.0947224
LM0= 0.0388712
WordPenalty0= -0.0296365
PhrasePenalty0= 0.0178253
TranslationModel0= -0.018859 0.0514054 0.336355 -0.0176808 0.2
UnknownWordPenalty0= 1
